{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9daa2818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script de entrenamiento BERT + MLflow para clasificaci√≥n de hate speech\n",
    "Dataset: 997 muestras, ~46% hate\n",
    "Objetivo: F1/Precision/Recall aceptables, overfitting < 5%\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8b649ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 1. CONFIGURACI√ìN Y PATHS\n",
    "# ============================================================================\n",
    "\n",
    "def setup_paths():\n",
    "    \"\"\"Configura los paths del proyecto\"\"\"\n",
    "    current_dir = Path.cwd()\n",
    "    \n",
    "    # Si estamos en notebooks, subir un nivel\n",
    "    if \"notebooks\" in str(current_dir):\n",
    "        project_root = current_dir.parent\n",
    "    else:\n",
    "        project_root = current_dir\n",
    "    \n",
    "    # Crear carpeta mlruns si no existe\n",
    "    mlruns_dir = project_root / \"mlruns\"\n",
    "    mlruns_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Path directo a data/processed\n",
    "    data_dir = project_root / \"data\" / \"processed\"\n",
    "    \n",
    "    return project_root, mlruns_dir, data_dir\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Fija todas las semillas para reproducibilidad\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f7040711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. DATA AUGMENTATION PARA CLASE MINORITARIA\n",
    "# ============================================================================\n",
    "\n",
    "def synonym_replacement(text, n=2):\n",
    "    \"\"\"\n",
    "    Augmentaci√≥n simple: intercambio aleatorio de palabras\n",
    "    (En producci√≥n usar nlpaug o backtranslation)\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) < 3:\n",
    "        return text\n",
    "    \n",
    "    # Intercambiar n palabras aleatorias\n",
    "    for _ in range(min(n, len(words) // 3)):\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def augment_minority_class(X, y, target_class=1, augment_factor=0.3):\n",
    "    \"\"\"\n",
    "    Aumenta la clase minoritaria con augmentaci√≥n ligera\n",
    "    \n",
    "    Args:\n",
    "        X: textos originales\n",
    "        y: labels\n",
    "        target_class: clase a aumentar (1 = hate)\n",
    "        augment_factor: porcentaje de nuevas muestras (0.3 = +30%)\n",
    "    \n",
    "    Returns:\n",
    "        X_aug, y_aug: datos aumentados\n",
    "    \"\"\"\n",
    "    minority_mask = y == target_class\n",
    "    minority_X = X[minority_mask]\n",
    "    minority_y = y[minority_mask]\n",
    "    \n",
    "    # Calcular cu√°ntas muestras crear\n",
    "    n_samples = int(len(minority_X) * augment_factor)\n",
    "    \n",
    "    # Muestreo aleatorio con reemplazo\n",
    "    indices = np.random.choice(len(minority_X), size=n_samples, replace=True)\n",
    "    \n",
    "    aug_texts = []\n",
    "    for idx in indices:\n",
    "        # Aplicar augmentaci√≥n\n",
    "        original_text = minority_X[idx]\n",
    "        aug_text = synonym_replacement(original_text, n=2)\n",
    "        aug_texts.append(aug_text)\n",
    "    \n",
    "    # Combinar originales + aumentados\n",
    "    X_aug = np.concatenate([X, np.array(aug_texts)])\n",
    "    y_aug = np.concatenate([y, np.full(n_samples, target_class)])\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5d26214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. DATASET Y DATALOADER\n",
    "# ============================================================================\n",
    "\n",
    "class HateSpeechDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para clasificaci√≥n de hate speech\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        # Tokenizaci√≥n\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "788ada92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. CARGA Y PREPARACI√ìN DE DATOS\n",
    "# ============================================================================\n",
    "\n",
    "def load_and_prepare_data(data_dir, use_augmentation=True):\n",
    "    \"\"\"\n",
    "    Carga el dataset y lo prepara para entrenamiento\n",
    "    \n",
    "    Args:\n",
    "        data_dir: directorio con los archivos pickle\n",
    "        use_augmentation: si aplicar augmentaci√≥n a clase minoritaria\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    # Cargar pickle directamente\n",
    "    pickle_path = data_dir / \"youtube_all_versions.pkl\"\n",
    "    \n",
    "    print(f\"üìÇ Cargando datos desde: {pickle_path}\")\n",
    "    df = pd.read_pickle(pickle_path)\n",
    "    \n",
    "    # Si es un dict, extraer DataFrame\n",
    "    if isinstance(df, dict):\n",
    "        print(f\"üì¶ Pickle es diccionario. Claves: {list(df.keys())}\")\n",
    "        # Usar la primera clave que sea DataFrame\n",
    "        for key, value in df.items():\n",
    "            if isinstance(value, pd.DataFrame):\n",
    "                df = value\n",
    "                print(f\"‚úÖ Usando DataFrame de clave: '{key}'\")\n",
    "                break\n",
    "    \n",
    "    print(f\"‚úÖ Columnas disponibles: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Buscar columna de texto\n",
    "    text_col = None\n",
    "    for col in ['Text_Lemmatized', 'Text_Cleaned', 'Text_Normalized', 'Text']:\n",
    "        if col in df.columns:\n",
    "            text_col = col\n",
    "            break\n",
    "    \n",
    "    print(f\"‚úÖ Usando columna: {text_col}\")\n",
    "    \n",
    "    # Filtrar nulos\n",
    "    df = df.dropna(subset=[text_col, 'IsHate'])\n",
    "    \n",
    "    X = df[text_col].astype(str).values\n",
    "    y = df['IsHate'].astype(int).values\n",
    "    \n",
    "    print(f\"\\nüìä Dataset: {len(X)} muestras\")\n",
    "    print(f\"   - Hate: {np.sum(y)} ({np.mean(y)*100:.1f}%)\")\n",
    "    print(f\"   - Normal: {len(y) - np.sum(y)} ({(1-np.mean(y))*100:.1f}%)\")\n",
    "    \n",
    "    # Split estratificado: 70% train, 15% val, 15% test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìà Split:\")\n",
    "    print(f\"   Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Val:   {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # Augmentaci√≥n\n",
    "    if use_augmentation:\n",
    "        print(\"\\nüîÑ Aplicando augmentaci√≥n...\")\n",
    "        X_train, y_train = augment_minority_class(\n",
    "            X_train, y_train, \n",
    "            target_class=1,\n",
    "            augment_factor=0.3\n",
    "        )\n",
    "        print(f\"   Train aumentado: {len(X_train)} muestras\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7d9e0a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 5. FUNCI√ìN DE ENTRENAMIENTO CON CLASS WEIGHT\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_class_weights(y_train, device):\n",
    "    \"\"\"Calcula pesos de clase balanceados\"\"\"\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    \n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.array([0, 1]),\n",
    "        y=y_train\n",
    "    )\n",
    "    return torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device, class_weights=None):\n",
    "    \"\"\"Entrena el modelo por una √©poca con class weights\"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        # Mover batch a device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        # Calcular loss con class weights si se proporcionan\n",
    "        if class_weights is not None:\n",
    "            logits = outputs.logits\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "        else:\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b0ea94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# 6. PIPELINE COMPLETO DE ENTRENAMIENTO CON FROZEN LAYERS Y CLASS WEIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def calculate_class_weights(y_train, device):\n",
    "    \"\"\"Calcula pesos de clase balanceados\"\"\"\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.array([0, 1]),\n",
    "        y=y_train\n",
    "    )\n",
    "    return torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "\n",
    "def train_bert_model(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    model_name='distilbert-base-uncased',\n",
    "    max_len=128,\n",
    "    batch_size=16,\n",
    "    epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    device=None,\n",
    "    freeze_base=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline completo de entrenamiento BERT con class weights y frozen layers\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(f\"\\nüöÄ Iniciando entrenamiento en: {device}\")\n",
    "    print(f\"   Modelo: {model_name}\")\n",
    "    print(f\"   Epochs: {epochs}, Batch size: {batch_size}, Max len: {max_len}\")\n",
    "    print(f\"   Freeze base: {freeze_base}\")\n",
    "    \n",
    "    # 1. Tokenizer\n",
    "    print(\"\\nüìù Cargando tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # 2. Datasets\n",
    "    train_dataset = HateSpeechDataset(X_train, y_train, tokenizer, max_len)\n",
    "    val_dataset = HateSpeechDataset(X_val, y_val, tokenizer, max_len)\n",
    "    \n",
    "    # 3. DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    \n",
    "    # 4. Modelo (compatible con BERT/DistilBERT/TinyBERT)\n",
    "    print(\"üß† Cargando modelo...\")\n",
    "    try:\n",
    "        # Intenta primero configuraci√≥n BERT/DistilBERT\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2,\n",
    "            dropout=0.3\n",
    "        ).to(device)\n",
    "    except:\n",
    "        # Fallback: configuraci√≥n b√°sica (TinyBERT)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2\n",
    "        ).to(device)\n",
    "\n",
    "    \n",
    "    # 5. CONGELAR CAPAS BASE si freeze_base=True\n",
    "    if freeze_base:\n",
    "        print(\"üîí Congelando capas base del transformer...\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'classifier' not in name and 'pre_classifier' not in name:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"   Par√°metros entrenables: {trainable:,} / {total:,} ({trainable/total*100:.1f}%)\")\n",
    "    \n",
    "    # 6. Class weights para balancear clases\n",
    "    class_weights = calculate_class_weights(y_train, device)\n",
    "    print(f\"‚öñÔ∏è  Class weights: Normal={class_weights[0]:.3f}, Hate={class_weights[1]:.3f}\")\n",
    "    \n",
    "    # 7. Optimizer y Scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # 8. Loss function con class weights\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # 9. Training loop con early stopping\n",
    "    print(\"\\n‚è≥ Entrenando modelo...\\n\")\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    best_epoch = 0\n",
    "    best_model_state = None\n",
    "    history = []\n",
    "    patience = 1\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=None\n",
    "            )\n",
    "            \n",
    "            # Loss con class weights\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        train_loss = np.mean(train_losses)\n",
    "        \n",
    "        # EVALUATE\n",
    "        val_metrics = eval_model(model, val_loader, device)\n",
    "        \n",
    "        # Log\n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_precision': val_metrics['precision'],\n",
    "            'val_recall': val_metrics['recall'],\n",
    "            'val_f1': val_metrics['f1']\n",
    "        })\n",
    "        \n",
    "        # Calcular overfitting\n",
    "        overfitting_gap = abs(train_loss - val_metrics['loss'])\n",
    "        overfitting_pct = (overfitting_gap / train_loss) * 100\n",
    "        \n",
    "        print(f\"   Train Loss:     {train_loss:.4f}\")\n",
    "        print(f\"   Val Loss:       {val_metrics['loss']:.4f}\")\n",
    "        print(f\"   Val Precision:  {val_metrics['precision']:.4f}\")\n",
    "        print(f\"   Val Recall:     {val_metrics['recall']:.4f}\")\n",
    "        print(f\"   Val F1:         {val_metrics['f1']:.4f}\")\n",
    "        print(f\"   Overfitting:    {overfitting_pct:.2f}% {'‚úÖ' if overfitting_pct < 5 else '‚ö†Ô∏è' if overfitting_pct < 15 else '‚ùå'}\")\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if val_metrics['f1'] > best_f1:\n",
    "            best_f1 = val_metrics['f1']\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"   üéØ Nuevo mejor F1: {best_f1:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\n‚èπÔ∏è  Early stopping en epoch {epoch + 1}\")\n",
    "                break\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Cargar mejor modelo\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\n‚úÖ Cargado mejor modelo (Epoch {best_epoch}, F1: {best_f1:.4f})\")\n",
    "    \n",
    "    return model, tokenizer, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6d9da373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. EVALUACI√ìN FINAL EN TEST\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_test_set(model, tokenizer, X_test, y_test, device, max_len=128):\n",
    "    \"\"\"Eval√∫a el modelo en el conjunto de test\"\"\"\n",
    "    test_dataset = HateSpeechDataset(X_test, y_test, tokenizer, max_len)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä EVALUACI√ìN EN TEST SET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    metrics = eval_model(model, test_loader, device)\n",
    "    \n",
    "    print(f\"\\n   Test Loss:      {metrics['loss']:.4f}\")\n",
    "    print(f\"   Test Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"   Test Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"   Test F1:        {metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Matriz de confusi√≥n\n",
    "    print(\"\\nüìà Confusion Matrix:\")\n",
    "    cm = confusion_matrix(metrics['true_labels'], metrics['predictions'])\n",
    "    print(cm)\n",
    "    \n",
    "    # Reporte de clasificaci√≥n\n",
    "    print(\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(\n",
    "        metrics['true_labels'], \n",
    "        metrics['predictions'],\n",
    "        target_names=['Normal', 'Hate']\n",
    "    ))\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "360a6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. INTEGRACI√ìN CON MLFLOW (CORREGIDO PARA WINDOWS)\n",
    "# ============================================================================\n",
    "\n",
    "def run_experiment_with_mlflow(\n",
    "    project_root,\n",
    "    mlruns_dir,\n",
    "    data_dir,\n",
    "    experiment_name=\"youtube_hate_speech_bert\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline completo con tracking en MLflow\n",
    "    \n",
    "    Args:\n",
    "        project_root: directorio ra√≠z del proyecto\n",
    "        mlruns_dir: directorio para mlruns\n",
    "        data_dir: directorio con los datos\n",
    "        experiment_name: nombre del experimento MLflow\n",
    "    \"\"\"\n",
    "    # Configurar MLflow - FIX DEFINITIVO PARA WINDOWS\n",
    "    # Convertir Path de Windows a URI v√°lida\n",
    "    tracking_uri = mlruns_dir.as_uri()\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    \n",
    "    print(f\"üîß MLflow Tracking URI: {tracking_uri}\")\n",
    "    \n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    # Fijar semilla\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Cargar datos\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_and_prepare_data(\n",
    "        data_dir, \n",
    "        use_augmentation=True\n",
    "    )\n",
    "    \n",
    "    # Hiperpar√°metros\n",
    "    # Hiperpar√°metros optimizados para dataset peque√±o con DistilBERT\n",
    "    # Hiperpar√°metros con TinyBERT (modelo m√°s peque√±o)\n",
    "    # Hiperpar√°metros CORREGIDOS para TinyBERT\n",
    "    # Hiperpar√°metros balanceados con DistilBERT\n",
    "    # Hiperpar√°metros TinyBERT con class_weights\n",
    "    # Hiperpar√°metros ULTRA conservadores\n",
    "    params = {\n",
    "        'model_name': 'huawei-noah/TinyBERT_General_4L_312D',\n",
    "        'max_len': 128,\n",
    "        'batch_size': 8,          # M√°s peque√±o\n",
    "        'epochs': 2,              # Solo 2 epochs (viste que epoch 3-4 explotan)\n",
    "        'learning_rate': 2e-5,    # LR muy bajo\n",
    "        'weight_decay': 0.05,     # Regularizaci√≥n m√°xima\n",
    "        'use_augmentation': True,\n",
    "        'augment_factor': 0.2,    # M√≠nima augmentaci√≥n\n",
    "        'seed': 42,\n",
    "        'freeze_base': False\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    # Iniciar run de MLflow\n",
    "    with mlflow.start_run(run_name=f\"bert_{params['max_len']}_{params['epochs']}ep\"):\n",
    "        \n",
    "        # Log de par√°metros\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        model, tokenizer, history = train_bert_model(\n",
    "            X_train, y_train,\n",
    "            X_val, y_val,\n",
    "            model_name=params['model_name'],\n",
    "            max_len=params['max_len'],\n",
    "            batch_size=params['batch_size'],\n",
    "            epochs=params['epochs'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            weight_decay=params['weight_decay'],\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Log de m√©tricas de validaci√≥n por √©poca\n",
    "        for epoch_metrics in history:\n",
    "            mlflow.log_metrics({\n",
    "                'train_loss': epoch_metrics['train_loss'],\n",
    "                'val_loss': epoch_metrics['val_loss'],\n",
    "                'val_precision': epoch_metrics['val_precision'],\n",
    "                'val_recall': epoch_metrics['val_recall'],\n",
    "                'val_f1': epoch_metrics['val_f1']\n",
    "            }, step=epoch_metrics['epoch'])\n",
    "        \n",
    "        # Evaluaci√≥n en test\n",
    "        test_metrics = evaluate_test_set(\n",
    "            model, tokenizer, X_test, y_test, device, params['max_len']\n",
    "        )\n",
    "        \n",
    "        # Log de m√©tricas finales\n",
    "        mlflow.log_metrics({\n",
    "            'test_loss': test_metrics['loss'],\n",
    "            'test_precision': test_metrics['precision'],\n",
    "            'test_recall': test_metrics['recall'],\n",
    "            'test_f1': test_metrics['f1']\n",
    "        })\n",
    "        \n",
    "        # Calcular overfitting final\n",
    "        final_train_loss = history[-1]['train_loss']\n",
    "        final_val_loss = history[-1]['val_loss']\n",
    "        overfitting_pct = abs(final_train_loss - final_val_loss) / final_train_loss * 100\n",
    "        mlflow.log_metric('overfitting_percentage', overfitting_pct)\n",
    "        \n",
    "        # Guardar tokenizer\n",
    "        tokenizer_dir = project_root / \"models\" / \"tokenizer\"\n",
    "        tokenizer_dir.mkdir(parents=True, exist_ok=True)\n",
    "        tokenizer.save_pretrained(tokenizer_dir)\n",
    "        mlflow.log_artifacts(str(tokenizer_dir), artifact_path=\"tokenizer\")\n",
    "        \n",
    "        # Guardar modelo en MLflow (sin registro en model registry para evitar problemas)\n",
    "        mlflow.pytorch.log_model(\n",
    "            model,\n",
    "            artifact_path=\"model\"\n",
    "            # Comentamos registered_model_name para evitar problemas con model registry\n",
    "            # registered_model_name=\"bert_hate_speech\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ EXPERIMENTO COMPLETADO\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"   MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "        print(f\"   Experiment ID: {mlflow.get_experiment_by_name(experiment_name).experiment_id}\")\n",
    "        print(f\"   Run ID: {mlflow.active_run().info.run_id}\")\n",
    "        print(f\"\\n   üìä M√©tricas finales:\")\n",
    "        print(f\"      Test F1: {test_metrics['f1']:.4f}\")\n",
    "        print(f\"      Overfitting: {overfitting_pct:.2f}%\")\n",
    "        print(f\"\\n   üíæ Artefactos guardados en: {mlruns_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f2bd9c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ü§ñ ENTRENAMIENTO DESTILBERT - HATE SPEECH DETECTION\n",
      "============================================================\n",
      "\n",
      "üìÅ Configuraci√≥n de paths:\n",
      "   Project root: c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\n",
      "   MLflow dir:   c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\mlruns\n",
      "   Data dir:     c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\data\\processed\n",
      "üîß MLflow Tracking URI: file:///c:/Users/Administrator/Desktop/NLP/Proyecto_X_NLP_Equipo3/mlruns\n",
      "üìÇ Cargando datos desde: c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\data\\processed\\youtube_all_versions.pkl\n",
      "‚úÖ Columnas disponibles: ['CommentId', 'VideoId', 'Text', 'IsToxic', 'IsAbusive', 'IsThreat', 'IsProvocative', 'IsObscene', 'IsHatespeech', 'IsRacist', 'IsNationalist', 'IsSexist', 'IsHomophobic', 'IsReligiousHate', 'IsRadicalism', 'IsHate', 'num_labels', 'char_count', 'word_count', 'sentence_count', 'avg_word_length', 'uppercase_count', 'uppercase_ratio', 'exclamation_count', 'question_count', 'dots_count', 'emoji_count', 'url_count', 'mention_count', 'hashtag_count', 'number_count', 'Text_Clean_Basic', 'char_count_clean', 'word_count_clean', 'Text_Clean_Advanced', 'Text_No_Stopwords', 'word_count_no_stop', 'Text_Stemmed', 'Text_Lemmatized']\n",
      "‚úÖ Usando columna: Text_Lemmatized\n",
      "\n",
      "üìä Dataset: 997 muestras\n",
      "   - Hate: 459 (46.0%)\n",
      "   - Normal: 538 (54.0%)\n",
      "\n",
      "üìà Split:\n",
      "   Train: 697 (69.9%)\n",
      "   Val:   150 (15.0%)\n",
      "   Test:  150 (15.0%)\n",
      "\n",
      "üîÑ Aplicando augmentaci√≥n...\n",
      "   Train aumentado: 793 muestras\n",
      "\n",
      "üöÄ Iniciando entrenamiento en: cpu\n",
      "   Modelo: huawei-noah/TinyBERT_General_4L_312D\n",
      "   Epochs: 2, Batch size: 8, Max len: 128\n",
      "   Freeze base: False\n",
      "\n",
      "üìù Cargando tokenizer...\n",
      "üß† Cargando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è  Class weights: Normal=1.055, Hate=0.951\n",
      "\n",
      "‚è≥ Entrenando modelo...\n",
      "\n",
      "============================================================\n",
      "Epoch 1/2\n",
      "============================================================\n",
      "   Train Loss:     0.6806\n",
      "   Val Loss:       0.6571\n",
      "   Val Precision:  0.7667\n",
      "   Val Recall:     0.3333\n",
      "   Val F1:         0.4646\n",
      "   Overfitting:    3.46% ‚úÖ\n",
      "   üéØ Nuevo mejor F1: 0.4646\n",
      "\n",
      "============================================================\n",
      "Epoch 2/2\n",
      "============================================================\n",
      "   Train Loss:     0.6003\n",
      "   Val Loss:       0.6229\n",
      "   Val Precision:  0.6076\n",
      "   Val Recall:     0.6957\n",
      "   Val F1:         0.6486\n",
      "   Overfitting:    3.77% ‚úÖ\n",
      "   üéØ Nuevo mejor F1: 0.6486\n",
      "\n",
      "\n",
      "‚úÖ Cargado mejor modelo (Epoch 2, F1: 0.6486)\n",
      "\n",
      "============================================================\n",
      "üìä EVALUACI√ìN EN TEST SET\n",
      "============================================================\n",
      "\n",
      "   Test Loss:      0.6082\n",
      "   Test Precision: 0.6375\n",
      "   Test Recall:    0.7391\n",
      "   Test F1:        0.6846\n",
      "\n",
      "üìà Confusion Matrix:\n",
      "[[52 29]\n",
      " [18 51]]\n",
      "\n",
      "üìã Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.74      0.64      0.69        81\n",
      "        Hate       0.64      0.74      0.68        69\n",
      "\n",
      "    accuracy                           0.69       150\n",
      "   macro avg       0.69      0.69      0.69       150\n",
      "weighted avg       0.69      0.69      0.69       150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/28 19:45:29 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/28 19:45:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ EXPERIMENTO COMPLETADO\n",
      "============================================================\n",
      "   MLflow Tracking URI: file:///c:/Users/Administrator/Desktop/NLP/Proyecto_X_NLP_Equipo3/mlruns\n",
      "   Experiment ID: 718288864077088622\n",
      "   Run ID: 9d4185ab6f664c00b174c9a04856592f\n",
      "\n",
      "   üìä M√©tricas finales:\n",
      "      Test F1: 0.6846\n",
      "      Overfitting: 3.77%\n",
      "\n",
      "   üíæ Artefactos guardados en: c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\mlruns\n",
      "\n",
      "‚ú® Entrenamiento finalizado con √©xito!\n",
      "\n",
      "üí° Para ver resultados en MLflow UI:\n",
      "   cd c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\n",
      "   mlflow ui --backend-store-uri file://c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\mlruns\n",
      "   Abrir: http://localhost:5000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 9. MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Funci√≥n principal\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ü§ñ ENTRENAMIENTO DESTILBERT - HATE SPEECH DETECTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Setup\n",
    "    project_root, mlruns_dir, data_dir = setup_paths()\n",
    "    \n",
    "    print(f\"\\nüìÅ Configuraci√≥n de paths:\")\n",
    "    print(f\"   Project root: {project_root}\")\n",
    "    print(f\"   MLflow dir:   {mlruns_dir}\")\n",
    "    print(f\"   Data dir:     {data_dir}\")\n",
    "    \n",
    "    # Ejecutar experimento\n",
    "    run_experiment_with_mlflow(\n",
    "        project_root=project_root,\n",
    "        mlruns_dir=mlruns_dir,\n",
    "        data_dir=data_dir,\n",
    "        experiment_name=\"youtube_hate_speech_bert\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚ú® Entrenamiento finalizado con √©xito!\")\n",
    "    print(f\"\\nüí° Para ver resultados en MLflow UI:\")\n",
    "    print(f\"   cd {project_root}\")\n",
    "    print(f\"   mlflow ui --backend-store-uri file://{mlruns_dir}\")\n",
    "    print(f\"   Abrir: http://localhost:5000\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
