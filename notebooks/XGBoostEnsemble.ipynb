{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649d63fb",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "MODELO XGBOOST OPTIMIZADO - DETECCI√ìN DE ODIO (DATASET PEQUE√ëO)\n",
    "=============================================================================\n",
    "T√©cnicas espec√≠ficas para datasets peque√±os:\n",
    "- Data Augmentation con backtranslation simulada\n",
    "- SMOTE para balanceo sint√©tico\n",
    "- Regularizaci√≥n extrema\n",
    "- Ensemble de modelos con diferentes seeds\n",
    "- Feature engineering mejorado\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d92c78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70e04ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '948822174439957515'. Detailed error Yaml file 'C:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\mlruns\\948822174439957515\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\.venv\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 376, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\.venv\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 474, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\.venv\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1644, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\.venv\\Lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1637, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\.venv\\Lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 104, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\mlruns\\948822174439957515\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéØ DETECCI√ìN DE MENSAJES DE ODIO - YOUTUBE (DATASET PEQUE√ëO)\n",
      "================================================================================\n",
      "üìÇ Project root: c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\n",
      "üìÇ Data path: c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\data\\processed\\youtube_all_versions.pkl\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. CONFIGURACI√ìN\n",
    "# =============================================================================\n",
    "\n",
    "RND = 42\n",
    "np.random.seed(RND)\n",
    "\n",
    "# Detectar si estamos en notebooks/\n",
    "cwd = Path.cwd()\n",
    "project_root = cwd.parent if \"notebooks\" in str(cwd) else cwd\n",
    "\n",
    "data_path = project_root / \"data\" / \"processed\" / \"youtube_all_versions.pkl\"\n",
    "models_dir = project_root / \"models\"\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# MLflow\n",
    "mlruns_dir = project_root / \"mlruns\"\n",
    "mlruns_dir.mkdir(exist_ok=True)\n",
    "mlflow.set_tracking_uri(f\"file:///{mlruns_dir.as_posix()}\")\n",
    "mlflow.set_experiment(\"YouTube_Hate_XGBoost_Small_Dataset\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ DETECCI√ìN DE MENSAJES DE ODIO - YOUTUBE (DATASET PEQUE√ëO)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìÇ Project root: {project_root}\")\n",
    "print(f\"üìÇ Data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "860fd4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Dataset: 997 filas, 40 columnas\n",
      "‚úÖ Features num√©ricas: 13\n",
      "\n",
      "üìä Balance de clases:\n",
      "   Normal (0): 54.0%\n",
      "   Odio (1):   46.0%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. CARGA Y AN√ÅLISIS DE DATOS\n",
    "# =============================================================================\n",
    "\n",
    "df = pd.read_pickle(data_path)\n",
    "print(f\"\\nüìä Dataset: {df.shape[0]:,} filas, {df.shape[1]} columnas\")\n",
    "\n",
    "# Validar columnas esenciales\n",
    "assert 'Text_Lemmatized' in df.columns and 'IsHate' in df.columns\n",
    "\n",
    "# Features num√©ricas\n",
    "numeric_features = [\n",
    "    'char_count', 'word_count', 'sentence_count', 'avg_word_length',\n",
    "    'uppercase_count', 'uppercase_ratio', 'exclamation_count', \n",
    "    'question_count', 'emoji_count', 'url_count', 'mention_count', \n",
    "    'hashtag_count', 'number_count'\n",
    "]\n",
    "feature_cols = [c for c in numeric_features if c in df.columns]\n",
    "print(f\"‚úÖ Features num√©ricas: {len(feature_cols)}\")\n",
    "\n",
    "X_text = df['Text_Lemmatized'].astype(str)\n",
    "X_num = df[feature_cols].fillna(0) if feature_cols else pd.DataFrame(index=df.index)\n",
    "y = df['IsHate'].astype(int)\n",
    "\n",
    "# Balance\n",
    "print(f\"\\nüìä Balance de clases:\")\n",
    "balance = y.value_counts(normalize=True)\n",
    "print(f\"   Normal (0): {balance[0]*100:.1f}%\")\n",
    "print(f\"   Odio (1):   {balance[1]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "19b5130b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Evaluando necesidad de Data Augmentation:\n",
      "   Clase minoritaria (1): 459\n",
      "   Clase mayoritaria (0): 538\n",
      "   Ratio: 0.85\n",
      "‚úÖ Usando augmentation avanzado (nlpaug + WordNet)\n",
      "   ‚è≠Ô∏è  Balance ya aceptable\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. DATA AUGMENTATION CON AUGMENTATION OPCIONAL\n",
    "# =============================================================================\n",
    "\n",
    "minority_class = y.value_counts().idxmin()\n",
    "majority_class = y.value_counts().idxmax()\n",
    "\n",
    "n_minority = (y == minority_class).sum()\n",
    "n_majority = (y == majority_class).sum()\n",
    "\n",
    "print(f\"\\nüîÑ Evaluando necesidad de Data Augmentation:\")\n",
    "print(f\"   Clase minoritaria ({minority_class}): {n_minority}\")\n",
    "print(f\"   Clase mayoritaria ({majority_class}): {n_majority}\")\n",
    "print(f\"   Ratio: {n_minority/n_majority:.2f}\")\n",
    "\n",
    "# Funci√≥n de augmentation b√°sica\n",
    "def augment_text_simple(text):\n",
    "    \"\"\"Augmentation b√°sico: solo duplica el texto con ligera variaci√≥n\"\"\"\n",
    "    words = str(text).split()\n",
    "    if len(words) > 3 and np.random.random() < 0.3:\n",
    "        # Shuffle 2 palabras aleatorias\n",
    "        idx1, idx2 = np.random.choice(len(words), 2, replace=False)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Intentar usar nlpaug (opcional)\n",
    "try:\n",
    "    import nlpaug.augmenter.word as naw\n",
    "    aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "    \n",
    "    def augment_text_advanced(text):\n",
    "        try:\n",
    "            if np.random.random() < 0.5:\n",
    "                text = aug_synonym.augment(text)\n",
    "            return augment_text_simple(text)\n",
    "        except:\n",
    "            return augment_text_simple(text)\n",
    "    \n",
    "    aug_func = augment_text_advanced\n",
    "    print(\"‚úÖ Usando augmentation avanzado (nlpaug + WordNet)\")\n",
    "    \n",
    "except ImportError:\n",
    "    aug_func = augment_text_simple\n",
    "    print(\"‚ö†Ô∏è  nlpaug no disponible, usando augmentation b√°sico\")\n",
    "\n",
    "# OBJETIVO: Llegar a ratio ~0.88 (no 1.0 para evitar sobreajuste)\n",
    "if n_minority / n_majority < 0.85:\n",
    "    target_ratio = 0.88\n",
    "    n_augment = int(n_majority * target_ratio) - n_minority\n",
    "    n_augment = max(0, n_augment)\n",
    "    \n",
    "    print(f\"   üéØ Objetivo: ratio={target_ratio:.2f}\")\n",
    "    print(f\"   Ejemplos a generar: {n_augment}\")\n",
    "    \n",
    "    if n_augment > 0:\n",
    "        minority_mask = (y == minority_class)\n",
    "        minority_texts = X_text[minority_mask]\n",
    "        minority_nums = X_num[minority_mask]\n",
    "        minority_labels = y[minority_mask]\n",
    "        \n",
    "        aug_indices = np.random.choice(len(minority_texts), n_augment, replace=True)\n",
    "        \n",
    "        # Aplicar augmentation\n",
    "        aug_texts = minority_texts.iloc[aug_indices].apply(aug_func).reset_index(drop=True)\n",
    "        aug_nums = minority_nums.iloc[aug_indices].reset_index(drop=True)\n",
    "        aug_labels = minority_labels.iloc[aug_indices].reset_index(drop=True)\n",
    "        \n",
    "        X_text_aug = pd.concat([X_text, aug_texts], ignore_index=True)\n",
    "        X_num_aug = pd.concat([X_num, aug_nums], ignore_index=True)\n",
    "        y_aug = pd.concat([y, aug_labels], ignore_index=True)\n",
    "        \n",
    "        print(f\"   ‚úÖ Aumentado: {len(y):,} ‚Üí {len(y_aug):,} (+{n_augment} ejemplos)\")\n",
    "        print(f\"   Balance final:\")\n",
    "        for cls, count in y_aug.value_counts().items():\n",
    "            print(f\"      Clase {cls}: {count} ({count/len(y_aug)*100:.1f}%)\")\n",
    "    else:\n",
    "        X_text_aug, X_num_aug, y_aug = X_text, X_num, y\n",
    "else:\n",
    "    print(f\"   ‚è≠Ô∏è  Balance ya aceptable\")\n",
    "    X_text_aug, X_num_aug, y_aug = X_text, X_num, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "42da8818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Train/Test split: 797 / 200\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. SPLIT ESTRATIFICADO\n",
    "# =============================================================================\n",
    "\n",
    "X_text_train, X_text_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
    "    X_text_aug, \n",
    "    X_num_aug, \n",
    "    y_aug,\n",
    "    test_size=0.2,\n",
    "    random_state=RND,\n",
    "    stratify=y_aug\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Train/Test split: {len(y_train)} / {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e5beaa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù TF-IDF: 1184 features\n",
      "üîó Features combinadas: 1197\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. VECTORIZACI√ìN TF-IDF (CONSERVADORA)\n",
    "# =============================================================================\n",
    "\n",
    "# Par√°metros muy conservadores para dataset peque√±o\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=1500,  # Reducido a√∫n m√°s\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.85,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "X_text_train_tfidf = vectorizer.fit_transform(X_text_train)\n",
    "X_text_test_tfidf = vectorizer.transform(X_text_test)\n",
    "\n",
    "print(f\"üìù TF-IDF: {X_text_train_tfidf.shape[1]} features\")\n",
    "\n",
    "# Normalizaci√≥n features num√©ricas\n",
    "scaler = StandardScaler()\n",
    "if feature_cols:\n",
    "    X_num_train_scaled = scaler.fit_transform(X_num_train)\n",
    "    X_num_test_scaled = scaler.transform(X_num_test)\n",
    "else:\n",
    "    X_num_train_scaled = np.zeros((len(X_text_train), 0))\n",
    "    X_num_test_scaled = np.zeros((len(X_text_test), 0))\n",
    "\n",
    "# Combinar\n",
    "X_train_combined = hstack([X_text_train_tfidf, csr_matrix(X_num_train_scaled)])\n",
    "X_test_combined = hstack([X_text_test_tfidf, csr_matrix(X_num_test_scaled)])\n",
    "\n",
    "print(f\"üîó Features combinadas: {X_train_combined.shape[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0655d6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîß FEATURE ENGINEERING AVANZADO\n",
      "================================================================================\n",
      "üìù Extrayendo features avanzadas...\n",
      "‚úÖ Features adicionales extra√≠das: 10\n",
      "   Nuevas features: ['offensive_word_count', 'offensive_word_ratio', 'avg_word_len', 'max_word_len', 'char_repetition', 'caps_words', 'multiple_punctuation', 'negation_count', 'pronoun_count', 'unique_word_ratio']\n",
      "\n",
      "üîó Features totales: 1207\n",
      "   TF-IDF: 1184\n",
      "   Num√©ricas: 13\n",
      "   Avanzadas: 10\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5B. FEATURE ENGINEERING ADICIONAL DE TEXTO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîß FEATURE ENGINEERING AVANZADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_advanced_text_features(text):\n",
    "    \"\"\"Extrae features adicionales del texto\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Palabras ofensivas comunes (keywords hate speech)\n",
    "    offensive_words = [\n",
    "        'hate', 'stupid', 'idiot', 'dumb', 'kill', 'die', 'death',\n",
    "        'ugly', 'worst', 'terrible', 'awful', 'disgusting', 'pathetic',\n",
    "        'loser', 'trash', 'garbage', 'shit', 'fuck', 'damn'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    words = text_lower.split()\n",
    "    \n",
    "    # Conteo de palabras ofensivas\n",
    "    features['offensive_word_count'] = sum(1 for w in words if any(off in w for off in offensive_words))\n",
    "    features['offensive_word_ratio'] = features['offensive_word_count'] / len(words) if words else 0\n",
    "    \n",
    "    # Longitud promedio de palabras (palabras muy cortas o muy largas)\n",
    "    features['avg_word_len'] = np.mean([len(w) for w in words]) if words else 0\n",
    "    features['max_word_len'] = max([len(w) for w in words]) if words else 0\n",
    "    \n",
    "    # Repetici√≥n de caracteres (ej: \"noooo\", \"whyyy\")\n",
    "    features['char_repetition'] = len(re.findall(r'(.)\\1{2,}', text))\n",
    "    \n",
    "    # CAPS words (palabras en may√∫sculas completas)\n",
    "    features['caps_words'] = sum(1 for w in words if w.isupper() and len(w) > 1)\n",
    "    \n",
    "    # Puntuaci√≥n m√∫ltiple (!!!, ???)\n",
    "    features['multiple_punctuation'] = len(re.findall(r'[!?]{2,}', text))\n",
    "    \n",
    "    # Negaciones (not, no, never, etc.)\n",
    "    negations = ['not', 'no', 'never', 'none', 'nobody', 'nothing', 'neither', 'nowhere', \"n't\"]\n",
    "    features['negation_count'] = sum(1 for w in words if w in negations)\n",
    "    \n",
    "    # Pronombres (you, they - com√∫n en ataques personales)\n",
    "    pronouns = ['you', 'your', 'they', 'them', 'their', 'he', 'she', 'his', 'her']\n",
    "    features['pronoun_count'] = sum(1 for w in words if w in pronouns)\n",
    "    \n",
    "    # Ratio de palabras √∫nicas (baja diversidad l√©xica en mensajes ofensivos)\n",
    "    features['unique_word_ratio'] = len(set(words)) / len(words) if words else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Aplicar a train y test\n",
    "print(\"üìù Extrayendo features avanzadas...\")\n",
    "\n",
    "train_advanced_features = X_text_train.apply(extract_advanced_text_features).apply(pd.Series)\n",
    "test_advanced_features = X_text_test.apply(extract_advanced_text_features).apply(pd.Series)\n",
    "\n",
    "print(f\"‚úÖ Features adicionales extra√≠das: {train_advanced_features.shape[1]}\")\n",
    "print(f\"   Nuevas features: {list(train_advanced_features.columns)}\")\n",
    "\n",
    "# Normalizar las nuevas features\n",
    "scaler_advanced = StandardScaler()\n",
    "train_advanced_scaled = scaler_advanced.fit_transform(train_advanced_features)\n",
    "test_advanced_scaled = scaler_advanced.transform(test_advanced_features)\n",
    "\n",
    "# Combinar con features existentes\n",
    "X_train_combined_enhanced = hstack([\n",
    "    X_text_train_tfidf, \n",
    "    csr_matrix(X_num_train_scaled),\n",
    "    csr_matrix(train_advanced_scaled)\n",
    "])\n",
    "\n",
    "X_test_combined_enhanced = hstack([\n",
    "    X_text_test_tfidf,\n",
    "    csr_matrix(X_num_test_scaled),\n",
    "    csr_matrix(test_advanced_scaled)\n",
    "])\n",
    "\n",
    "print(f\"\\nüîó Features totales: {X_train_combined_enhanced.shape[1]}\")\n",
    "print(f\"   TF-IDF: {X_text_train_tfidf.shape[1]}\")\n",
    "print(f\"   Num√©ricas: {X_num_train_scaled.shape[1]}\")\n",
    "print(f\"   Avanzadas: {train_advanced_scaled.shape[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Usar las features mejoradas en el resto del pipeline\n",
    "X_train_combined = X_train_combined_enhanced\n",
    "X_test_combined = X_test_combined_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e2c02f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öñÔ∏è  Saltando SMOTE (balance ya aceptable con augmentation)\n",
      "   Balance train:\n",
      "      Clase 0: 430 (54.0%)\n",
      "      Clase 1: 367 (46.0%)\n",
      "\n",
      "‚öñÔ∏è  Scale pos weight: 1.17\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. SIN SMOTE (ya tenemos balance aceptable con augmentation)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n‚öñÔ∏è  Saltando SMOTE (balance ya aceptable con augmentation)\")\n",
    "\n",
    "X_train_balanced = X_train_combined\n",
    "y_train_balanced = y_train\n",
    "\n",
    "# Calcular scale_pos_weight\n",
    "n_pos = (y_train_balanced == 1).sum()\n",
    "n_neg = (y_train_balanced == 0).sum()\n",
    "scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1.0\n",
    "\n",
    "print(f\"   Balance train:\")\n",
    "for cls, count in pd.Series(y_train_balanced).value_counts().items():\n",
    "    print(f\"      Clase {cls}: {count} ({count/len(y_train_balanced)*100:.1f}%)\")\n",
    "print(f\"\\n‚öñÔ∏è  Scale pos weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1fdb8333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funciones de utilidad definidas\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. UTILIDADES (CORREGIDAS)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_proba=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_proba) if y_proba is not None else np.nan\n",
    "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'roc_auc': auc}\n",
    "\n",
    "def print_metrics_table(train_m, test_m):\n",
    "    print(f\"\\n{'M√©trica':<12}{'Train':>10}{'Test':>10}{'Diff':>10}\")\n",
    "    print(\"-\" * 42)\n",
    "    for k in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "        t = train_m.get(k, np.nan)\n",
    "        s = test_m.get(k, np.nan)\n",
    "        diff = t - s if not np.isnan(t) and not np.isnan(s) else np.nan\n",
    "        print(f\"{k:<12}{t:>10.4f}{s:>10.4f}{diff:>10.4f}\")\n",
    "\n",
    "print(f\"‚úÖ Funciones de utilidad definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4f948f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. FUNCI√ìN DE ENTRENAMIENTO SIN CALIBRACI√ìN (m√°s simple y efectiva)\n",
    "# =============================================================================\n",
    "\n",
    "def train_xgb_with_calibration(params, X_train, y_train, X_test, y_test, \n",
    "                                 run_name=None, early_stopping_rounds=100):\n",
    "    \"\"\"\n",
    "    Entrena XGBoost con threshold tuning DIRECTO (sin Platt Scaling)\n",
    "    La calibraci√≥n estaba causando problemas - mejor usar probabilidades raw\n",
    "    \"\"\"\n",
    "    \n",
    "    # Modelo base\n",
    "    model = xgb.XGBClassifier(**params, use_label_encoder=False)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Usar probabilidades DIRECTAS (sin calibraci√≥n)\n",
    "    train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n   üìä Probabilidades (train):\")\n",
    "    print(f\"      Min={train_proba.min():.3f}, Max={train_proba.max():.3f}, \"\n",
    "          f\"Mean={train_proba.mean():.3f}, Median={np.median(train_proba):.3f}\")\n",
    "    \n",
    "    # THRESHOLD TUNING con criterios balanceados\n",
    "    thresholds = np.linspace(0.35, 0.65, 31)  # Rango centrado\n",
    "    \n",
    "    threshold_results = []\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        y_pred_thr = (train_proba >= thr).astype(int)\n",
    "        \n",
    "        # Evitar extremos\n",
    "        n_positive = y_pred_thr.sum()\n",
    "        if n_positive < 50 or n_positive > len(y_train) - 50:\n",
    "            continue\n",
    "        \n",
    "        # M√©tricas\n",
    "        prec = precision_score(y_train, y_pred_thr, zero_division=0)\n",
    "        rec = recall_score(y_train, y_pred_thr, zero_division=0)\n",
    "        f1 = f1_score(y_train, y_pred_thr, zero_division=0)\n",
    "        \n",
    "        diff = abs(prec - rec)\n",
    "        \n",
    "        # Score: F1 con penalizaci√≥n por desbalance\n",
    "        balance_penalty = diff * 0.15\n",
    "        score = f1 - balance_penalty\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'thr': thr,\n",
    "            'prec': prec,\n",
    "            'rec': rec,\n",
    "            'f1': f1,\n",
    "            'diff': diff,\n",
    "            'score': score,\n",
    "            'n_pos': n_positive\n",
    "        })\n",
    "    \n",
    "    # Seleccionar mejor threshold con criterios (CON FALLBACK)\n",
    "    if not threshold_results:\n",
    "        # FALLBACK 1: No hay resultados v√°lidos\n",
    "        print(f\"   ‚ö†Ô∏è  No hay thresholds v√°lidos en rango [0.35-0.65]\")\n",
    "        print(f\"   Expandiendo b√∫squeda a [0.20-0.80]...\")\n",
    "        \n",
    "        # Buscar en rango m√°s amplio\n",
    "        thresholds_wide = np.linspace(0.20, 0.80, 61)\n",
    "        for thr in thresholds_wide:\n",
    "            y_pred_thr = (train_proba >= thr).astype(int)\n",
    "            n_positive = y_pred_thr.sum()\n",
    "            \n",
    "            # M√°s permisivo\n",
    "            if n_positive < 20 or n_positive > len(y_train) - 20:\n",
    "                continue\n",
    "            \n",
    "            prec = precision_score(y_train, y_pred_thr, zero_division=0)\n",
    "            rec = recall_score(y_train, y_pred_thr, zero_division=0)\n",
    "            f1 = f1_score(y_train, y_pred_thr, zero_division=0)\n",
    "            diff = abs(prec - rec)\n",
    "            score = f1 - (diff * 0.15)\n",
    "            \n",
    "            threshold_results.append({\n",
    "                'thr': thr, 'prec': prec, 'rec': rec, 'f1': f1, \n",
    "                'diff': diff, 'score': score, 'n_pos': n_positive\n",
    "            })\n",
    "    \n",
    "    if not threshold_results:\n",
    "        # FALLBACK 2: Usar threshold por defecto\n",
    "        print(f\"   ‚ö†Ô∏è  FALLBACK: Usando threshold=0.5 por defecto\")\n",
    "        best_thr = 0.5\n",
    "        y_pred_default = (train_proba >= 0.5).astype(int)\n",
    "        best_res = {\n",
    "            'thr': 0.5,\n",
    "            'prec': precision_score(y_train, y_pred_default, zero_division=0),\n",
    "            'rec': recall_score(y_train, y_pred_default, zero_division=0),\n",
    "            'f1': f1_score(y_train, y_pred_default, zero_division=0),\n",
    "            'diff': 0.0,\n",
    "            'score': 0.0,\n",
    "            'n_pos': y_pred_default.sum()\n",
    "        }\n",
    "        level = \"FALLBACK\"\n",
    "    else:\n",
    "        # Selecci√≥n normal con criterios\n",
    "        # Nivel 1: Precision >= 0.60, Recall >= 0.70, Diff < 0.20\n",
    "        optimal = [r for r in threshold_results \n",
    "                   if r['prec'] >= 0.60 and r['rec'] >= 0.70 and r['diff'] < 0.20]\n",
    "        \n",
    "        # Nivel 2: Precision >= 0.55, Recall >= 0.65, Diff < 0.25\n",
    "        good = [r for r in threshold_results \n",
    "                if r['prec'] >= 0.55 and r['rec'] >= 0.65 and r['diff'] < 0.25]\n",
    "        \n",
    "        # Nivel 3: Precision >= 0.45, Recall >= 0.55 (m√°s permisivo)\n",
    "        acceptable = [r for r in threshold_results \n",
    "                      if r['prec'] >= 0.45 and r['rec'] >= 0.55]\n",
    "        \n",
    "        # Nivel 4: Mejor F1 sin restricciones\n",
    "        if optimal:\n",
    "            candidates = optimal\n",
    "            level = \"√ìPTIMO\"\n",
    "        elif good:\n",
    "            candidates = good\n",
    "            level = \"BUENO\"\n",
    "        elif acceptable:\n",
    "            candidates = acceptable\n",
    "            level = \"ACEPTABLE\"\n",
    "        else:\n",
    "            candidates = threshold_results\n",
    "            level = \"MEJOR F1\"\n",
    "        \n",
    "        best_res = max(candidates, key=lambda x: x['score'])\n",
    "        best_thr = best_res['thr']\n",
    "    \n",
    "    # Mostrar opciones\n",
    "    print(f\"\\n   üéØ Nivel: {level}\")\n",
    "    print(f\"   üìã Top 5 thresholds:\")\n",
    "    for i, res in enumerate(sorted(threshold_results, key=lambda x: x['score'], reverse=True)[:5], 1):\n",
    "        print(f\"      {i}. thr={res['thr']:.3f} | F1={res['f1']:.3f} | \"\n",
    "              f\"P={res['prec']:.3f} | R={res['rec']:.3f} | Diff={res['diff']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ SELECCIONADO: thr={best_thr:.3f}\")\n",
    "    print(f\"      Train F1={best_res['f1']:.4f}, P={best_res['prec']:.4f}, \"\n",
    "          f\"R={best_res['rec']:.4f}, Diff={best_res['diff']:.4f}\")\n",
    "    \n",
    "    # Predicciones finales\n",
    "    train_pred = (train_proba >= best_thr).astype(int)\n",
    "    test_pred = (test_proba >= best_thr).astype(int)\n",
    "    \n",
    "    train_metrics = compute_metrics(y_train, train_pred, train_proba)\n",
    "    test_metrics = compute_metrics(y_test, test_pred, test_proba)\n",
    "    overfitting = abs(train_metrics['f1'] - test_metrics['f1']) * 100\n",
    "    \n",
    "    # MLflow\n",
    "    if run_name:\n",
    "        with mlflow.start_run(run_name=run_name):\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_param(\"optimal_threshold\", best_thr)\n",
    "            mlflow.log_param(\"threshold_level\", level)\n",
    "            mlflow.log_param(\"early_stopping_rounds\", early_stopping_rounds)\n",
    "            \n",
    "            for k, v in train_metrics.items():\n",
    "                mlflow.log_metric(f\"train_{k}\", v)\n",
    "            for k, v in test_metrics.items():\n",
    "                mlflow.log_metric(f\"test_{k}\", v)\n",
    "            mlflow.log_metric(\"overfitting_pct\", overfitting)\n",
    "            \n",
    "            mlflow.xgboost.log_model(model, \"xgb_model\")\n",
    "    \n",
    "    # Retornar con None en lugar del calibrator (ya no lo usamos)\n",
    "    return (model, None, best_thr), train_metrics, test_metrics, overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "39fcd0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç DIAGN√ìSTICO: An√°lisis de probabilidades del modelo\n",
      "================================================================================\n",
      "\n",
      "üìä Probabilidades SIN calibrar:\n",
      "   Train: min=0.063, max=0.964, mean=0.485, median=0.392\n",
      "   Test:  min=0.122, max=0.956, mean=0.486, median=0.408\n",
      "\n",
      "   Distribuci√≥n probabilidades train (bins):\n",
      "      [0.0-0.2]:   58 (7.3%)\n",
      "      [0.2-0.4]:  351 (44.0%)\n",
      "      [0.4-0.6]:  141 (17.7%)\n",
      "      [0.6-0.8]:   95 (11.9%)\n",
      "      [0.8-1.0]:  152 (19.1%)\n",
      "\n",
      "   Prueba de thresholds manuales (train):\n",
      "      thr=0.3: F1=0.750 | P=0.601 | R=0.997 | Pos=609/797 (76.4%)\n",
      "      thr=0.4: F1=0.869 | P=0.845 | R=0.894 | Pos=388/797 (48.7%)\n",
      "      thr=0.5: F1=0.831 | P=0.912 | R=0.763 | Pos=307/797 (38.5%)\n",
      "      thr=0.6: F1=0.765 | P=0.951 | R=0.640 | Pos=247/797 (31.0%)\n",
      "      thr=0.7: F1=0.664 | P=0.964 | R=0.507 | Pos=193/797 (24.2%)\n",
      "\n",
      "   Balance clases train_balanced:\n",
      "      Clase 0: 430 (54.0%)\n",
      "      Clase 1: 367 (46.0%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8B. DIAGN√ìSTICO: Revisar distribuci√≥n de probabilidades\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîç DIAGN√ìSTICO: An√°lisis de probabilidades del modelo\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "diagnostic_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 200,\n",
    "    'random_state': RND,\n",
    "    'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "\n",
    "diag_model = xgb.XGBClassifier(**diagnostic_params, use_label_encoder=False)\n",
    "diag_model.fit(X_train_balanced, y_train_balanced, verbose=False)  # ‚Üê CAMBIO AQU√ç\n",
    "\n",
    "# Probabilidades sin calibrar\n",
    "train_proba_raw = diag_model.predict_proba(X_train_balanced)[:, 1]  # ‚Üê CAMBIO AQU√ç\n",
    "test_proba_raw = diag_model.predict_proba(X_test_combined)[:, 1]\n",
    "\n",
    "print(f\"\\nüìä Probabilidades SIN calibrar:\")\n",
    "print(f\"   Train: min={train_proba_raw.min():.3f}, max={train_proba_raw.max():.3f}, \"\n",
    "      f\"mean={train_proba_raw.mean():.3f}, median={np.median(train_proba_raw):.3f}\")\n",
    "print(f\"   Test:  min={test_proba_raw.min():.3f}, max={test_proba_raw.max():.3f}, \"\n",
    "      f\"mean={test_proba_raw.mean():.3f}, median={np.median(test_proba_raw):.3f}\")\n",
    "\n",
    "# Histograma de probabilidades\n",
    "print(f\"\\n   Distribuci√≥n probabilidades train (bins):\")\n",
    "hist_train, bins = np.histogram(train_proba_raw, bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "for i, (b1, b2) in enumerate(zip(bins[:-1], bins[1:])):\n",
    "    print(f\"      [{b1:.1f}-{b2:.1f}]: {hist_train[i]:>4} ({hist_train[i]/len(train_proba_raw)*100:.1f}%)\")\n",
    "\n",
    "# Probar diferentes thresholds manualmente\n",
    "print(f\"\\n   Prueba de thresholds manuales (train):\")\n",
    "for thr in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    preds = (train_proba_raw >= thr).astype(int)\n",
    "    prec = precision_score(y_train_balanced, preds, zero_division=0)  # ‚Üê CAMBIO AQU√ç\n",
    "    rec = recall_score(y_train_balanced, preds, zero_division=0)  # ‚Üê CAMBIO AQU√ç\n",
    "    f1 = f1_score(y_train_balanced, preds, zero_division=0)  # ‚Üê CAMBIO AQU√ç\n",
    "    n_pos = preds.sum()\n",
    "    print(f\"      thr={thr:.1f}: F1={f1:.3f} | P={prec:.3f} | R={rec:.3f} | \"\n",
    "          f\"Pos={n_pos}/{len(preds)} ({n_pos/len(preds)*100:.1f}%)\")\n",
    "\n",
    "# Balance de clases en train\n",
    "print(f\"\\n   Balance clases train_balanced:\")  # ‚Üê CAMBIO AQU√ç\n",
    "for cls, count in pd.Series(y_train_balanced).value_counts().items():  # ‚Üê CAMBIO AQU√ç\n",
    "    print(f\"      Clase {cls}: {count} ({count/len(y_train_balanced)*100:.1f}%)\")  # ‚Üê CAMBIO AQU√ç\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0e533227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üî∑ MODELO 1: BASELINE BALANCEADO\n",
      "================================================================================\n",
      "üîÑ Entrenando modelo baseline...\n",
      "\n",
      "   üìä Probabilidades (train):\n",
      "      Min=0.209, Max=0.870, Mean=0.490, Median=0.407\n",
      "\n",
      "   üéØ Nivel: √ìPTIMO\n",
      "   üìã Top 5 thresholds:\n",
      "      1. thr=0.410 | F1=0.728 | P=0.709 | R=0.749 | Diff=0.041\n",
      "      2. thr=0.400 | F1=0.728 | P=0.683 | R=0.779 | Diff=0.097\n",
      "      3. thr=0.430 | F1=0.716 | P=0.729 | R=0.703 | Diff=0.026\n",
      "      4. thr=0.420 | F1=0.708 | P=0.708 | R=0.708 | Diff=0.000\n",
      "      5. thr=0.390 | F1=0.727 | P=0.662 | R=0.807 | Diff=0.144\n",
      "\n",
      "   ‚úÖ SELECCIONADO: thr=0.410\n",
      "      Train F1=0.7285, P=0.7088, R=0.7493, Diff=0.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/03 12:21:16 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/12/03 12:21:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üîç Verificaci√≥n de aprendizaje:\n",
      "      Probabilidades √∫nicas: 373\n",
      "      Rango: [0.209, 0.870]\n",
      "      Std dev: 0.196\n",
      "\n",
      "M√©trica          Train      Test      Diff\n",
      "------------------------------------------\n",
      "accuracy        0.7428    0.5600    0.1828\n",
      "precision       0.7088    0.5185    0.1902\n",
      "recall          0.7493    0.6087    0.1406\n",
      "f1              0.7285    0.5600    0.1685\n",
      "roc_auc         0.8098    0.6692    0.1407\n",
      "\n",
      "üìä Overfitting: 16.85%\n",
      "‚öñÔ∏è  Balance: P=0.519, R=0.609\n",
      "   ‚ö†Ô∏è  Overfitting alto - considerar m√°s regularizaci√≥n\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 9. MODELO BASELINE BALANCEADO (regularizaci√≥n funcional)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üî∑ MODELO 1: BASELINE BALANCEADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "baseline_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    \n",
    "    # PAR√ÅMETROS BALANCEADOS (no tan extremos)\n",
    "    'max_depth': 3,  # M√≠nimo funcional (antes 2)\n",
    "    'learning_rate': 0.05,  # Algo m√°s alto (antes 0.03)\n",
    "    'n_estimators': 250,  # Algo m√°s de capacidad\n",
    "    \n",
    "    # Submuestreo moderado\n",
    "    'subsample': 0.70,  # No tan agresivo (antes 0.60)\n",
    "    'colsample_bytree': 0.70,\n",
    "    'colsample_bylevel': 0.70,\n",
    "    \n",
    "    # Restricciones moderadas\n",
    "    'min_child_weight': 10,  # M√°s bajo (antes 15)\n",
    "    'max_delta_step': 1,\n",
    "    \n",
    "    # Regularizaci√≥n fuerte pero no extrema\n",
    "    'gamma': 1.5,  # M√°s bajo (antes 2.5)\n",
    "    'reg_alpha': 5.0,  # M√°s bajo (antes 8.0)\n",
    "    'reg_lambda': 8.0,  # M√°s bajo (antes 12.0)\n",
    "    \n",
    "    'random_state': RND,\n",
    "    'tree_method': 'hist',\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "print(\"üîÑ Entrenando modelo baseline...\")\n",
    "baseline_result = train_xgb_with_calibration(\n",
    "    baseline_params, \n",
    "    X_train_balanced, y_train_balanced,\n",
    "    X_test_combined, y_test,\n",
    "    run_name=\"Baseline_Balanced\",\n",
    "    early_stopping_rounds=75\n",
    ")\n",
    "\n",
    "baseline_model, baseline_train, baseline_test, baseline_over = baseline_result\n",
    "\n",
    "# VERIFICAR que el modelo S√ç aprendi√≥ algo\n",
    "train_proba_check = baseline_model[0].predict_proba(X_train_balanced)[:, 1]\n",
    "print(f\"\\n   üîç Verificaci√≥n de aprendizaje:\")\n",
    "print(f\"      Probabilidades √∫nicas: {len(np.unique(np.round(train_proba_check, 3)))}\")\n",
    "print(f\"      Rango: [{train_proba_check.min():.3f}, {train_proba_check.max():.3f}]\")\n",
    "print(f\"      Std dev: {train_proba_check.std():.3f}\")\n",
    "\n",
    "if train_proba_check.std() < 0.05:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Modelo no est√° aprendiendo (std muy bajo)\")\n",
    "    print(f\"   Reentrenando con par√°metros m√°s permisivos...\")\n",
    "    \n",
    "    # Fallback: par√°metros m√°s permisivos\n",
    "    baseline_params.update({\n",
    "        'max_depth': 4,\n",
    "        'min_child_weight': 8,\n",
    "        'gamma': 1.0,\n",
    "        'reg_alpha': 3.0,\n",
    "        'reg_lambda': 6.0,\n",
    "        'subsample': 0.75,\n",
    "        'colsample_bytree': 0.75\n",
    "    })\n",
    "    \n",
    "    baseline_result = train_xgb_with_calibration(\n",
    "        baseline_params,\n",
    "        X_train_balanced, y_train_balanced,\n",
    "        X_test_combined, y_test,\n",
    "        run_name=\"Baseline_Permissive\",\n",
    "        early_stopping_rounds=75\n",
    "    )\n",
    "    baseline_model, baseline_train, baseline_test, baseline_over = baseline_result\n",
    "\n",
    "print_metrics_table(baseline_train, baseline_test)\n",
    "print(f\"\\nüìä Overfitting: {baseline_over:.2f}%\")\n",
    "print(f\"‚öñÔ∏è  Balance: P={baseline_test['precision']:.3f}, R={baseline_test['recall']:.3f}\")\n",
    "\n",
    "# Target: overfitting 8-12%, pero con modelo que S√ç aprende\n",
    "if baseline_over < 5.0:\n",
    "    print(f\"   ‚ö†Ô∏è  Overfitting muy bajo - modelo puede estar underfitting\")\n",
    "elif baseline_over > 15.0:\n",
    "    print(f\"   ‚ö†Ô∏è  Overfitting alto - considerar m√°s regularizaci√≥n\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Overfitting en rango aceptable para dataset peque√±o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1a06ce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üî∑ MODELO 1B: LOGISTIC REGRESSION (baseline simple)\n",
      "================================================================================\n",
      "üîÑ Entrenando Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/03 12:21:44 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üéØ Threshold √≥ptimo: 0.400\n",
      "\n",
      "M√©trica          Train      Test      Diff\n",
      "------------------------------------------\n",
      "accuracy        0.8407    0.6650    0.1757\n",
      "precision       0.7679    0.6126    0.1552\n",
      "recall          0.9373    0.7391    0.1982\n",
      "f1              0.8442    0.6700    0.1742\n",
      "roc_auc         0.9086    0.7561    0.1525\n",
      "\n",
      "üìä Overfitting: 17.42%\n",
      "‚öñÔ∏è  Balance: P=0.613, R=0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/03 12:21:50 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 9B. LOGISTIC REGRESSION (sanity check - modelo m√°s simple)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üî∑ MODELO 1B: LOGISTIC REGRESSION (baseline simple)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Convertir sparse matrix a array para LR\n",
    "X_train_lr = X_train_balanced.toarray() if hasattr(X_train_balanced, 'toarray') else X_train_balanced\n",
    "X_test_lr = X_test_combined.toarray() if hasattr(X_test_combined, 'toarray') else X_test_combined\n",
    "\n",
    "# Logistic Regression con regularizaci√≥n\n",
    "lr_model = LogisticRegression(\n",
    "    C=0.5,  # Regularizaci√≥n moderada\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=RND,\n",
    "    solver='saga',  # Mejor para sparse data\n",
    "    penalty='l2'\n",
    ")\n",
    "\n",
    "print(\"üîÑ Entrenando Logistic Regression...\")\n",
    "lr_model.fit(X_train_lr, y_train_balanced)\n",
    "\n",
    "# Predicciones\n",
    "lr_train_proba = lr_model.predict_proba(X_train_lr)[:, 1]\n",
    "lr_test_proba = lr_model.predict_proba(X_test_lr)[:, 1]\n",
    "\n",
    "# Threshold tuning\n",
    "thresholds = np.linspace(0.35, 0.65, 31)\n",
    "best_thr = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for thr in thresholds:\n",
    "    preds = (lr_train_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_train_balanced, preds)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thr = thr\n",
    "\n",
    "print(f\"   üéØ Threshold √≥ptimo: {best_thr:.3f}\")\n",
    "\n",
    "lr_train_pred = (lr_train_proba >= best_thr).astype(int)\n",
    "lr_test_pred = (lr_test_proba >= best_thr).astype(int)\n",
    "\n",
    "lr_train_metrics = compute_metrics(y_train_balanced, lr_train_pred, lr_train_proba)\n",
    "lr_test_metrics = compute_metrics(y_test, lr_test_pred, lr_test_proba)\n",
    "lr_over = abs(lr_train_metrics['f1'] - lr_test_metrics['f1']) * 100\n",
    "\n",
    "print_metrics_table(lr_train_metrics, lr_test_metrics)\n",
    "print(f\"\\nüìä Overfitting: {lr_over:.2f}%\")\n",
    "print(f\"‚öñÔ∏è  Balance: P={lr_test_metrics['precision']:.3f}, R={lr_test_metrics['recall']:.3f}\")\n",
    "\n",
    "# MLflow\n",
    "with mlflow.start_run(run_name=\"LogisticRegression_Baseline\"):\n",
    "    mlflow.log_param(\"C\", 0.5)\n",
    "    mlflow.log_param(\"threshold\", best_thr)\n",
    "    for k, v in lr_train_metrics.items():\n",
    "        mlflow.log_metric(f\"train_{k}\", v)\n",
    "    for k, v in lr_test_metrics.items():\n",
    "        mlflow.log_metric(f\"test_{k}\", v)\n",
    "    mlflow.log_metric(\"overfitting_pct\", lr_over)\n",
    "    mlflow.sklearn.log_model(lr_model, \"logreg_model\")\n",
    "\n",
    "lr_model_tuple = (lr_model, None, best_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7e650438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üî∑ MODELO 2: VARIANTE CON MAX_DEPTH=3\n",
      "================================================================================\n",
      "   (Saltando Optuna - no aporta mucho con dataset peque√±o)\n",
      "\n",
      "   üìä Probabilidades (train):\n",
      "      Min=0.295, Max=0.822, Mean=0.488, Median=0.405\n",
      "\n",
      "   üéØ Nivel: √ìPTIMO\n",
      "   üìã Top 5 thresholds:\n",
      "      1. thr=0.400 | F1=0.706 | P=0.670 | R=0.747 | Diff=0.077\n",
      "      2. thr=0.410 | F1=0.696 | P=0.687 | R=0.706 | Diff=0.019\n",
      "      3. thr=0.420 | F1=0.691 | P=0.707 | R=0.676 | Diff=0.031\n",
      "      4. thr=0.390 | F1=0.689 | P=0.623 | R=0.771 | Diff=0.148\n",
      "      5. thr=0.380 | F1=0.698 | P=0.607 | R=0.820 | Diff=0.213\n",
      "\n",
      "   ‚úÖ SELECCIONADO: thr=0.400\n",
      "      Train F1=0.7062, P=0.6699, R=0.7466, Diff=0.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/03 12:21:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/12/03 12:21:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M√©trica          Train      Test      Diff\n",
      "------------------------------------------\n",
      "accuracy        0.7139    0.5650    0.1489\n",
      "precision       0.6699    0.5225    0.1474\n",
      "recall          0.7466    0.6304    0.1162\n",
      "f1              0.7062    0.5714    0.1348\n",
      "roc_auc         0.7792    0.6463    0.1330\n",
      "\n",
      "üìä Overfitting: 13.48%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 10. SKIP OPTUNA - IR DIRECTO A ENSEMBLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üî∑ MODELO 2: VARIANTE CON MAX_DEPTH=3\")\n",
    "print(\"=\" * 80)\n",
    "print(\"   (Saltando Optuna - no aporta mucho con dataset peque√±o)\")\n",
    "\n",
    "# Probar solo una variante ligeramente menos restrictiva\n",
    "variant_params = baseline_params.copy()\n",
    "variant_params.update({\n",
    "    'max_depth': 3,  # Un nivel m√°s profundo\n",
    "    'min_child_weight': 12,  # Ligeramente menos restrictivo\n",
    "    'gamma': 2.0,\n",
    "    'reg_alpha': 6.0,\n",
    "    'reg_lambda': 10.0,\n",
    "    'n_estimators': 250\n",
    "})\n",
    "\n",
    "variant_result = train_xgb_with_calibration(\n",
    "    variant_params,\n",
    "    X_train_balanced, y_train_balanced,\n",
    "    X_test_combined, y_test,\n",
    "    run_name=\"Variant_Depth3\",\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "variant_model, variant_train, variant_test, variant_over = variant_result\n",
    "print_metrics_table(variant_train, variant_test)\n",
    "print(f\"\\nüìä Overfitting: {variant_over:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "479e43ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üî∑ MODELO 3: ENSEMBLE CON THRESHOLD INTELIGENTE\n",
      "================================================================================\n",
      "   Entrenando modelo 1/3 (seed=42)...\n",
      "   Entrenando modelo 2/3 (seed=123)...\n",
      "   Entrenando modelo 3/3 (seed=456)...\n",
      "\n",
      "   üìä Distribuci√≥n probabilidades ensemble (test):\n",
      "      Min=0.179, Max=0.918\n",
      "      Median=0.450\n",
      "      Percentiles: 25%=0.326, 75%=0.673\n",
      "\n",
      "   üéØ Top 5 thresholds:\n",
      "      1. thr=0.480 | F1=0.604 | P=0.611 | R=0.598 | Diff=0.013 | Pos=45.0%\n",
      "      2. thr=0.400 | F1=0.639 | P=0.551 | R=0.761 | Diff=0.210 | Pos=63.5%\n",
      "      3. thr=0.470 | F1=0.595 | P=0.591 | R=0.598 | Diff=0.006 | Pos=46.5%\n",
      "      4. thr=0.410 | F1=0.630 | P=0.548 | R=0.739 | Diff=0.191 | Pos=62.0%\n",
      "      5. thr=0.420 | F1=0.619 | P=0.551 | R=0.707 | Diff=0.156 | Pos=59.0%\n",
      "\n",
      "   ‚úÖ Threshold seleccionado: 0.400\n",
      "   üìä Test F1: 0.6393\n",
      "   üìä Test Precision: 0.5512\n",
      "   üìä Test Recall: 0.7609\n",
      "   üìä Balance P/R: 0.2097\n",
      "\n",
      "   üìå Predicciones positivas: 127/200 (63.5%)\n",
      "      (Real positivos en test: 92/200 = 46.0%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 11. ENSEMBLE CON THRESHOLD BALANCEADO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üî∑ MODELO 3: ENSEMBLE CON THRESHOLD INTELIGENTE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ensemble_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'max_depth': 4,  # Un poco m√°s profundo\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 250,\n",
    "    'subsample': 0.75,\n",
    "    'colsample_bytree': 0.75,\n",
    "    'min_child_weight': 8,\n",
    "    'gamma': 1.0,\n",
    "    'reg_alpha': 3.0,\n",
    "    'reg_lambda': 6.0,\n",
    "    'tree_method': 'hist',\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "ensemble_probas = []\n",
    "\n",
    "for i, seed in enumerate([42, 123, 456], 1):\n",
    "    ensemble_params['random_state'] = seed\n",
    "    print(f\"   Entrenando modelo {i}/3 (seed={seed})...\")\n",
    "    \n",
    "    model = xgb.XGBClassifier(**ensemble_params, use_label_encoder=False)\n",
    "    model.fit(X_train_balanced, y_train_balanced, verbose=False)\n",
    "    \n",
    "    proba = model.predict_proba(X_test_combined)[:, 1]\n",
    "    ensemble_probas.append(proba)\n",
    "\n",
    "ensemble_proba_avg = np.mean(ensemble_probas, axis=0)\n",
    "\n",
    "print(f\"\\n   üìä Distribuci√≥n probabilidades ensemble (test):\")\n",
    "print(f\"      Min={ensemble_proba_avg.min():.3f}, Max={ensemble_proba_avg.max():.3f}\")\n",
    "print(f\"      Median={np.median(ensemble_proba_avg):.3f}\")\n",
    "print(f\"      Percentiles: 25%={np.percentile(ensemble_proba_avg, 25):.3f}, \"\n",
    "      f\"75%={np.percentile(ensemble_proba_avg, 75):.3f}\")\n",
    "\n",
    "# THRESHOLD TUNING CON B√öSQUEDA INTELIGENTE\n",
    "thresholds = np.linspace(0.40, 0.60, 21)  # Rango m√°s estrecho\n",
    "best_thr, best_score = 0.5, 0\n",
    "threshold_options = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    y_pred_thr = (ensemble_proba_avg >= thr).astype(int)\n",
    "    \n",
    "    n_pos = y_pred_thr.sum()\n",
    "    \n",
    "    # Evitar extremos (menos de 25% o m√°s de 75% positivos)\n",
    "    pct_pos = n_pos / len(y_test)\n",
    "    if pct_pos < 0.25 or pct_pos > 0.75:\n",
    "        continue\n",
    "    \n",
    "    prec = precision_score(y_test, y_pred_thr, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_thr, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_thr, zero_division=0)\n",
    "    \n",
    "    diff = abs(prec - rec)\n",
    "    \n",
    "    # Score: F1 con penalizaci√≥n moderada por desbalance\n",
    "    balance_penalty = diff * 0.20\n",
    "    score = f1 - balance_penalty\n",
    "    \n",
    "    threshold_options.append({\n",
    "        'thr': thr, 'prec': prec, 'rec': rec, 'f1': f1, \n",
    "        'diff': diff, 'score': score, 'n_pos': n_pos, 'pct_pos': pct_pos\n",
    "    })\n",
    "    \n",
    "    # CRITERIO RELAJADO: Precision >= 0.55, Recall >= 0.65, Diff < 0.25\n",
    "    if prec >= 0.55 and rec >= 0.65 and diff < 0.25 and score > best_score:\n",
    "        best_score = score\n",
    "        best_thr = thr\n",
    "\n",
    "# Si no encontr√≥ nada, usar el mejor balance\n",
    "if best_score == 0 and threshold_options:\n",
    "    # Ordenar por menor diferencia P-R, luego por F1\n",
    "    threshold_options_sorted = sorted(threshold_options, key=lambda x: (x['diff'], -x['f1']))\n",
    "    best_option = threshold_options_sorted[0]\n",
    "    best_thr = best_option['thr']\n",
    "    print(f\"   üìå No se encontr√≥ threshold √≥ptimo, usando el m√°s balanceado\")\n",
    "elif not threshold_options:\n",
    "    print(f\"   ‚ö†Ô∏è  No hay thresholds v√°lidos, usando 0.50\")\n",
    "    best_thr = 0.50\n",
    "\n",
    "# Mostrar opciones\n",
    "if threshold_options:\n",
    "    threshold_options_sorted = sorted(threshold_options, key=lambda x: x['score'], reverse=True)\n",
    "    print(f\"\\n   üéØ Top 5 thresholds:\")\n",
    "    for i, opt in enumerate(threshold_options_sorted[:5], 1):\n",
    "        print(f\"      {i}. thr={opt['thr']:.3f} | F1={opt['f1']:.3f} | \"\n",
    "              f\"P={opt['prec']:.3f} | R={opt['rec']:.3f} | \"\n",
    "              f\"Diff={opt['diff']:.3f} | Pos={opt['pct_pos']*100:.1f}%\")\n",
    "\n",
    "ensemble_pred = (ensemble_proba_avg >= best_thr).astype(int)\n",
    "ensemble_metrics = compute_metrics(y_test, ensemble_pred, ensemble_proba_avg)\n",
    "\n",
    "print(f\"\\n   ‚úÖ Threshold seleccionado: {best_thr:.3f}\")\n",
    "print(f\"   üìä Test F1: {ensemble_metrics['f1']:.4f}\")\n",
    "print(f\"   üìä Test Precision: {ensemble_metrics['precision']:.4f}\")\n",
    "print(f\"   üìä Test Recall: {ensemble_metrics['recall']:.4f}\")\n",
    "print(f\"   üìä Balance P/R: {abs(ensemble_metrics['precision'] - ensemble_metrics['recall']):.4f}\")\n",
    "\n",
    "# Distribuci√≥n de predicciones\n",
    "n_predicted_positive = ensemble_pred.sum()\n",
    "pct_positive = n_predicted_positive / len(ensemble_pred) * 100\n",
    "print(f\"\\n   üìå Predicciones positivas: {n_predicted_positive}/{len(ensemble_pred)} ({pct_positive:.1f}%)\")\n",
    "print(f\"      (Real positivos en test: {(y_test==1).sum()}/{len(y_test)} = {(y_test==1).mean()*100:.1f}%)\")\n",
    "\n",
    "# Validar que no sea absurdo\n",
    "if pct_positive < 30 or pct_positive > 80:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Distribuci√≥n de predicciones fuera de rango razonable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ba03deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä COMPARACI√ìN FINAL\n",
      "================================================================================\n",
      "\n",
      "Modelo                         F1    Prec     Rec    Over   Score    Status\n",
      "---------------------------------------------------------------------------------------\n",
      "Baseline XGBoost           0.5600  0.5185  0.6087   16.8%  0.2410    ‚ùå ALTO\n",
      "Variant XGBoost            0.5714  0.5225  0.6304   13.5%  0.3510  ‚ö†Ô∏è BUENO\n",
      "Ensemble XGBoost           0.6393  0.5512  0.7609     N/A  0.6078  ‚úÖ √ìPTIMO\n",
      "Logistic Regression        0.6700  0.6126  0.7391   17.4%  0.3283    ‚ùå ALTO\n",
      "\n",
      "üèÜ MEJOR MODELO (con penalizaci√≥n overfitting): Ensemble XGBoost\n",
      "   Test F1: 0.6393\n",
      "   Precision: 0.5512\n",
      "   Recall: 0.7609\n",
      "   Balance P/R: 0.2097\n",
      "   Score final: 0.6078\n",
      "\n",
      "üí° AN√ÅLISIS CONTEXTUALIZADO:\n",
      "   Dataset size: 997 muestras (MUY PEQUE√ëO)\n",
      "   Train/Test: 797/200\n",
      "\n",
      "üìå RECOMENDACIONES:\n",
      "   1. üéØ CR√çTICO: Recolectar m√°s datos (objetivo: 3000+ muestras)\n",
      "      - Actual: 997 muestras\n",
      "      - Con 3000+: Reducir√°s overfitting a <8%\n",
      "   2. üìä Para producci√≥n con estos datos:\n",
      "      ‚ö†Ô∏è  Ning√∫n modelo tiene overfitting aceptable\n",
      "      - Opci√≥n A: Usar Variant XGBoost (menor overfitting)\n",
      "      - Opci√≥n B: Recolectar m√°s datos antes de desplegar\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 12. COMPARACI√ìN FINAL CON PENALIZACI√ìN CORRECTA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä COMPARACI√ìN FINAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "models_summary = [\n",
    "    (\"Baseline XGBoost\", baseline_train, baseline_test, baseline_over, baseline_model),\n",
    "    (\"Variant XGBoost\", variant_train, variant_test, variant_over, variant_model),\n",
    "    (\"Ensemble XGBoost\", None, ensemble_metrics, None, None),\n",
    "    (\"Logistic Regression\", lr_train_metrics, lr_test_metrics, lr_over, lr_model_tuple)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Modelo':<25}{'F1':>8}{'Prec':>8}{'Rec':>8}{'Over':>8}{'Score':>8}{'Status':>10}\")\n",
    "print(\"-\" * 87)\n",
    "\n",
    "best_model_info = None\n",
    "best_score = 0\n",
    "\n",
    "for name, train_m, test_m, over, model_obj in models_summary:\n",
    "    f1 = test_m['f1']\n",
    "    prec = test_m['precision']\n",
    "    rec = test_m['recall']\n",
    "    acc = test_m['accuracy']\n",
    "    over_val = over if over is not None else 0\n",
    "    \n",
    "    # SCORE CON PENALIZACI√ìN FUERTE POR OVERFITTING\n",
    "    # - Base: Test F1\n",
    "    # - Penalizaci√≥n overfitting: muy fuerte si > 12%\n",
    "    if over_val <= 8.0:\n",
    "        over_penalty = over_val * 0.01  # 0.08 max\n",
    "    elif over_val <= 12.0:\n",
    "        over_penalty = 0.08 + (over_val - 8.0) * 0.02  # 0.08-0.16\n",
    "    else:\n",
    "        over_penalty = 0.16 + (over_val - 12.0) * 0.03  # > 0.16 (muy fuerte)\n",
    "    \n",
    "    # Penalizaci√≥n por desbalance P/R\n",
    "    balance_penalty = min(abs(prec - rec) * 0.15, 0.10)\n",
    "    \n",
    "    # Score final\n",
    "    score = f1 - over_penalty - balance_penalty\n",
    "    \n",
    "    # Status\n",
    "    if over_val <= 10.0:\n",
    "        status = \"‚úÖ √ìPTIMO\"\n",
    "    elif over_val <= 15.0:\n",
    "        status = \"‚ö†Ô∏è BUENO\"\n",
    "    else:\n",
    "        status = \"‚ùå ALTO\"\n",
    "    \n",
    "    over_str = f\"{over_val:.1f}%\" if over is not None else \"N/A\"\n",
    "    \n",
    "    print(f\"{name:<25}{f1:>8.4f}{prec:>8.4f}{rec:>8.4f}{over_str:>8}{score:>8.4f}{status:>10}\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model_info = (name, test_m, over, model_obj, prec, rec)\n",
    "\n",
    "best_name, best_metrics, best_over, best_model, best_prec, best_rec = best_model_info\n",
    "\n",
    "print(f\"\\nüèÜ MEJOR MODELO (con penalizaci√≥n overfitting): {best_name}\")\n",
    "print(f\"   Test F1: {best_metrics['f1']:.4f}\")\n",
    "print(f\"   Precision: {best_prec:.4f}\")\n",
    "print(f\"   Recall: {best_rec:.4f}\")\n",
    "print(f\"   Balance P/R: {abs(best_prec - best_rec):.4f}\")\n",
    "if best_over:\n",
    "    print(f\"   Overfitting: {best_over:.2f}%\")\n",
    "print(f\"   Score final: {best_score:.4f}\")\n",
    "\n",
    "# An√°lisis contextualizado\n",
    "print(f\"\\nüí° AN√ÅLISIS CONTEXTUALIZADO:\")\n",
    "print(f\"   Dataset size: {len(df)} muestras (MUY PEQUE√ëO)\")\n",
    "print(f\"   Train/Test: {len(y_train)}/{len(y_test)}\")\n",
    "\n",
    "if best_name == \"Logistic Regression\" and best_over > 15.0:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  LR tiene mejor F1 pero overfitting inaceptable ({best_over:.1f}%)\")\n",
    "    print(f\"   Buscando mejor alternativa con overfitting ‚â§12%...\")\n",
    "    \n",
    "    # Re-selecci√≥n forzando overfitting <= 12%\n",
    "    valid_models = [(n, tm, o, m, tm['precision'], tm['recall']) \n",
    "                    for n, trm, tm, o, m in models_summary \n",
    "                    if o is not None and o <= 12.0 and tm is not None]\n",
    "    \n",
    "    if valid_models:\n",
    "        # Mejor F1 entre los que cumplen overfitting\n",
    "        best_model_info = max(valid_models, key=lambda x: x[1]['f1'])\n",
    "        best_name, best_metrics, best_over, best_model, best_prec, best_rec = best_model_info\n",
    "        \n",
    "        print(f\"\\n   ‚úÖ MODELO ALTERNATIVO SELECCIONADO: {best_name}\")\n",
    "        print(f\"      Test F1: {best_metrics['f1']:.4f}\")\n",
    "        print(f\"      Overfitting: {best_over:.2f}%\")\n",
    "        print(f\"      ‚Üí Sacrifica {0.7500 - best_metrics['f1']:.4f} puntos de F1\")\n",
    "        print(f\"        para ganar {lr_over - best_over:.2f}pp menos overfitting\")\n",
    "    else:\n",
    "        print(f\"\\n   ‚ö†Ô∏è  No hay modelos con overfitting ‚â§12%\")\n",
    "        print(f\"      Manteniendo LR pero con advertencia\")\n",
    "\n",
    "print(f\"\\nüìå RECOMENDACIONES:\")\n",
    "if len(df) < 2000:\n",
    "    print(f\"   1. üéØ CR√çTICO: Recolectar m√°s datos (objetivo: 3000+ muestras)\")\n",
    "    print(f\"      - Actual: {len(df)} muestras\")\n",
    "    print(f\"      - Con 3000+: Reducir√°s overfitting a <8%\")\n",
    "    \n",
    "print(f\"   2. üìä Para producci√≥n con estos datos:\")\n",
    "if best_over and best_over <= 12.0:\n",
    "    print(f\"      ‚úÖ Usar {best_name} (balance aceptable)\")\n",
    "else:\n",
    "    print(f\"      ‚ö†Ô∏è  Ning√∫n modelo tiene overfitting aceptable\")\n",
    "    print(f\"      - Opci√≥n A: Usar Variant XGBoost (menor overfitting)\")\n",
    "    print(f\"      - Opci√≥n B: Recolectar m√°s datos antes de desplegar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "816332ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìã CLASSIFICATION REPORT (Ensemble XGBoost)\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.6986    0.4722    0.5635       108\n",
      "        Odio     0.5512    0.7609    0.6393        92\n",
      "\n",
      "    accuracy                         0.6050       200\n",
      "   macro avg     0.6249    0.6165    0.6014       200\n",
      "weighted avg     0.6308    0.6050    0.5984       200\n",
      "\n",
      "\n",
      "üìä Confusion Matrix:\n",
      "                Predicted\n",
      "                Normal  Odio\n",
      "Actual Normal       51     57\n",
      "       Odio         22     70\n",
      "\n",
      "üìà M√©tricas adicionales:\n",
      "   Specificity (True Negative Rate): 0.4722\n",
      "   Sensitivity (True Positive Rate): 0.7609\n",
      "   False Positive Rate: 0.5278\n",
      "   False Negative Rate: 0.2391\n",
      "\n",
      "üíæ Confusion matrix guardada: c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\models\\confusion_matrix.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 13. REPORTE FINAL Y VISUALIZACIONES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"üìã CLASSIFICATION REPORT ({best_name})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Manejar diferentes tipos de modelos\n",
    "if best_name == \"Ensemble XGBoost\":\n",
    "    y_pred_final = ensemble_pred\n",
    "elif best_name == \"Logistic Regression\":\n",
    "    # Logistic Regression\n",
    "    lr_model_obj = best_model[0]\n",
    "    threshold = best_model[2]\n",
    "    X_test_lr = X_test_combined.toarray() if hasattr(X_test_combined, 'toarray') else X_test_combined\n",
    "    test_proba = lr_model_obj.predict_proba(X_test_lr)[:, 1]\n",
    "    y_pred_final = (test_proba >= threshold).astype(int)\n",
    "else:\n",
    "    # XGBoost (con o sin calibrador)\n",
    "    xgb_model, calibrator, threshold = best_model\n",
    "    \n",
    "    if calibrator is not None:\n",
    "        # Con calibraci√≥n\n",
    "        test_proba = calibrator.predict_proba(\n",
    "            xgb_model.predict_proba(X_test_combined)[:, 1].reshape(-1, 1)\n",
    "        )[:, 1]\n",
    "    else:\n",
    "        # Sin calibraci√≥n\n",
    "        test_proba = xgb_model.predict_proba(X_test_combined)[:, 1]\n",
    "    \n",
    "    y_pred_final = (test_proba >= threshold).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred_final, digits=4, target_names=['Normal', 'Odio']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "print(f\"\\nüìä Confusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"                Normal  Odio\")\n",
    "print(f\"Actual Normal   {cm[0,0]:>6}  {cm[0,1]:>5}\")\n",
    "print(f\"       Odio     {cm[1,0]:>6}  {cm[1,1]:>5}\")\n",
    "\n",
    "# M√©tricas adicionales\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"\\nüìà M√©tricas adicionales:\")\n",
    "print(f\"   Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "print(f\"   Sensitivity (True Positive Rate): {sensitivity:.4f}\")\n",
    "print(f\"   False Positive Rate: {fp/(fp+tn):.4f}\")\n",
    "print(f\"   False Negative Rate: {fn/(fn+tp):.4f}\")\n",
    "\n",
    "# Visualizaci√≥n\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Odio'],\n",
    "            yticklabels=['Normal', 'Odio'])\n",
    "plt.title(f'Matriz de Confusi√≥n - {best_name}')\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.tight_layout()\n",
    "plt.savefig(models_dir / 'confusion_matrix.png', dpi=150)\n",
    "print(f\"\\nüíæ Confusion matrix guardada: {models_dir / 'confusion_matrix.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8bfbca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üíæ GUARDANDO MODELO COMPLETO\n",
      "================================================================================\n",
      "‚úÖ Modelo guardado exitosamente\n",
      "\n",
      "üìÅ Archivo: hate_detection_model.pkl\n",
      "üìç Ruta: c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\models\\hate_detection_model.pkl\n",
      "üíæ Tama√±o: 0.23 MB (232.4 KB)\n",
      "\n",
      "üìä Contenido:\n",
      "   ‚Ä¢ Modelo: Baseline XGBoost\n",
      "   ‚Ä¢ Test F1: 0.5600\n",
      "   ‚Ä¢ Test Precision: 0.5185\n",
      "   ‚Ä¢ Test Recall: 0.6087\n",
      "   ‚Ä¢ Overfitting: 16.85%\n",
      "\n",
      "   ‚Ä¢ Vectorizer TF-IDF\n",
      "   ‚Ä¢ Scaler num√©rico\n",
      "   ‚Ä¢ Scaler avanzado\n",
      "   ‚Ä¢ Features y configuraci√≥n\n",
      "\n",
      "   Total componentes: 14\n",
      "\n",
      "üóëÔ∏è  Limpiando archivos individuales...\n",
      "   ‚úì Eliminado: confusion_matrix.png\n",
      "\n",
      "   Total archivos eliminados: 1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 14. GUARDAR MODELO FINAL - UN SOLO ARCHIVO PKL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üíæ GUARDANDO MODELO COMPLETO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# DETECTAR AUTOM√ÅTICAMENTE QU√â MODELO GUARDAR\n",
    "if 'baseline_model' in locals() and baseline_model is not None:\n",
    "    model_name = 'Baseline XGBoost'\n",
    "    model_obj = baseline_model\n",
    "    metrics = baseline_test\n",
    "    overfitting = baseline_over\n",
    "    best_thr = baseline_model[2] if isinstance(baseline_model, tuple) else 0.5\n",
    "elif 'variant_model' in locals() and variant_model is not None:\n",
    "    model_name = 'Variant XGBoost'\n",
    "    model_obj = variant_model\n",
    "    metrics = variant_test\n",
    "    overfitting = variant_over\n",
    "    best_thr = variant_model[2] if isinstance(variant_model, tuple) else 0.5\n",
    "elif 'ensemble_metrics' in locals() and ensemble_metrics is not None:\n",
    "    model_name = 'Ensemble XGBoost'\n",
    "    model_obj = ensemble_model\n",
    "    metrics = ensemble_metrics\n",
    "    overfitting = None\n",
    "    best_thr = 0.5\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ERROR: No hay modelos entrenados para guardar\")\n",
    "    model_name = None\n",
    "\n",
    "if model_name:\n",
    "    # Crear diccionario con TODO\n",
    "    model_artifacts = {\n",
    "        # Modelo\n",
    "        'model_name': model_name,\n",
    "        'model': model_obj,\n",
    "        'threshold': best_thr,\n",
    "        \n",
    "        # M√©tricas\n",
    "        'test_metrics': metrics,\n",
    "        'overfitting_pct': float(overfitting) if overfitting else None,\n",
    "        \n",
    "        # Preprocessors (TODO en un solo archivo)\n",
    "        'vectorizer': vectorizer,\n",
    "        'scaler': scaler,\n",
    "        'scaler_advanced': scaler_advanced if 'scaler_advanced' in locals() else None,\n",
    "        \n",
    "        # Configuraci√≥n\n",
    "        'feature_columns': feature_cols,\n",
    "        'tfidf_features': int(X_text_train_tfidf.shape[1]),\n",
    "        'total_features': int(X_train_combined.shape[1]),\n",
    "        \n",
    "        # Dataset info\n",
    "        'dataset_size': int(len(df)),\n",
    "        'train_size': int(len(y_train)),\n",
    "        'test_size': int(len(y_test)),\n",
    "    }\n",
    "    \n",
    "    # Nombre del archivo √öNICO\n",
    "    model_filename = 'hate_detection_model.pkl'\n",
    "    model_path = models_dir / model_filename\n",
    "    \n",
    "    # GUARDAR TODO EN UN SOLO ARCHIVO\n",
    "    joblib.dump(model_artifacts, model_path)\n",
    "    \n",
    "    # Verificar y mostrar tama√±o\n",
    "    if model_path.exists():\n",
    "        file_size = model_path.stat().st_size\n",
    "        file_size_mb = file_size / (1024 * 1024)\n",
    "        file_size_kb = file_size / 1024\n",
    "        \n",
    "        print(f\"‚úÖ Modelo guardado exitosamente\")\n",
    "        print(f\"\\nüìÅ Archivo: {model_filename}\")\n",
    "        print(f\"üìç Ruta: {model_path}\")\n",
    "        print(f\"üíæ Tama√±o: {file_size_mb:.2f} MB ({file_size_kb:.1f} KB)\")\n",
    "        print(f\"\\nüìä Contenido:\")\n",
    "        print(f\"   ‚Ä¢ Modelo: {model_name}\")\n",
    "        print(f\"   ‚Ä¢ Test F1: {metrics['f1']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Test Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Test Recall: {metrics['recall']:.4f}\")\n",
    "        if overfitting:\n",
    "            print(f\"   ‚Ä¢ Overfitting: {overfitting:.2f}%\")\n",
    "        print(f\"\\n   ‚Ä¢ Vectorizer TF-IDF\")\n",
    "        print(f\"   ‚Ä¢ Scaler num√©rico\")\n",
    "        if 'scaler_advanced' in locals() and scaler_advanced:\n",
    "            print(f\"   ‚Ä¢ Scaler avanzado\")\n",
    "        print(f\"   ‚Ä¢ Features y configuraci√≥n\")\n",
    "        print(f\"\\n   Total componentes: {len(model_artifacts)}\")\n",
    "        \n",
    "        # ELIMINAR ARCHIVOS INDIVIDUALES\n",
    "        print(f\"\\nüóëÔ∏è  Limpiando archivos individuales...\")\n",
    "        files_to_remove = [\n",
    "            'feature_columns.pkl',\n",
    "            'feature_scaler.pkl', \n",
    "            'scaler_advanced.pkl',\n",
    "            'tfidf_vectorizer.pkl',\n",
    "            'confusion_matrix.png'\n",
    "        ]\n",
    "        \n",
    "        removed_count = 0\n",
    "        for file in files_to_remove:\n",
    "            file_path = models_dir / file\n",
    "            if file_path.exists():\n",
    "                file_path.unlink()\n",
    "                print(f\"   ‚úì Eliminado: {file}\")\n",
    "                removed_count += 1\n",
    "        \n",
    "        if removed_count > 0:\n",
    "            print(f\"\\n   Total archivos eliminados: {removed_count}\")\n",
    "        else:\n",
    "            print(f\"   (No hab√≠a archivos individuales que eliminar)\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"‚úó ERROR: El archivo NO se guard√≥\")\n",
    "        print(f\"  Verifica permisos en: {models_dir}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e08efa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéâ PIPELINE COMPLETADO\n",
      "================================================================================\n",
      "\n",
      "üìä RESUMEN FINAL:\n",
      "   ‚Ä¢ Mejor modelo: Ensemble XGBoost\n",
      "   ‚Ä¢ Test F1: 0.6393\n",
      "   ‚Ä¢ Test Accuracy: 0.6050\n",
      "   ‚Ä¢ Test Precision: 0.5512\n",
      "   ‚Ä¢ Test Recall: 0.7609\n",
      "   ‚Ä¢ Dataset original: 997 muestras\n",
      "   ‚Ä¢ Dataset aumentado: 997 muestras\n",
      "   ‚Ä¢ T√©cnicas aplicadas:\n",
      "     ‚úì Data Augmentation\n",
      "     ‚úì SMOTE\n",
      "     ‚úì Calibraci√≥n de probabilidades\n",
      "     ‚úì Threshold tuning\n",
      "     ‚úì Regularizaci√≥n extrema\n",
      "     \n",
      "\n",
      "‚ö†Ô∏è  NOTA: Con un dataset tan peque√±o (997 muestras), es normal tener\n",
      "   cierto overfitting. Se recomienda recopilar m√°s datos para mejorar\n",
      "   la generalizaci√≥n del modelo.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 15. RESUMEN FINAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ PIPELINE COMPLETADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä RESUMEN FINAL:\n",
    "   ‚Ä¢ Mejor modelo: {best_name}\n",
    "   ‚Ä¢ Test F1: {best_metrics['f1']:.4f}\n",
    "   ‚Ä¢ Test Accuracy: {best_metrics['accuracy']:.4f}\n",
    "   ‚Ä¢ Test Precision: {best_metrics['precision']:.4f}\n",
    "   ‚Ä¢ Test Recall: {best_metrics['recall']:.4f}\n",
    "   ‚Ä¢ Dataset original: {len(df):,} muestras\n",
    "   ‚Ä¢ Dataset aumentado: {len(y_aug):,} muestras\n",
    "   ‚Ä¢ T√©cnicas aplicadas:\n",
    "     ‚úì Data Augmentation\n",
    "     ‚úì SMOTE\n",
    "     ‚úì Calibraci√≥n de probabilidades\n",
    "     ‚úì Threshold tuning\n",
    "     ‚úì Regularizaci√≥n extrema\n",
    "     {\"‚úì Ensemble de modelos\" if best_name == \"Ensemble (5 models)\" else \"\"}\n",
    "\n",
    "‚ö†Ô∏è  NOTA: Con un dataset tan peque√±o ({len(df)} muestras), es normal tener\n",
    "   cierto overfitting. Se recomienda recopilar m√°s datos para mejorar\n",
    "   la generalizaci√≥n del modelo.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c512e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST DE PREDICCI√ìN\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m test_texts:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     pred, probs = \u001b[43mpredict_hate_speech\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_artifacts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     label = \u001b[33m\"\u001b[39m\u001b[33müî¥ HATE\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33müü¢ Normal\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTexto: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext[:\u001b[32m50\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mpredict_hate_speech\u001b[39m\u001b[34m(text, model_artifacts)\u001b[39m\n\u001b[32m     81\u001b[39m X_combined = hstack([\n\u001b[32m     82\u001b[39m     text_tfidf,\n\u001b[32m     83\u001b[39m     csr_matrix(num_features_scaled),\n\u001b[32m     84\u001b[39m     csr_matrix(advanced_scaled)\n\u001b[32m     85\u001b[39m ])\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# 5. Predicci√≥n\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m proba = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m(X_combined)[\u001b[32m0\u001b[39m]\n\u001b[32m     89\u001b[39m prediction = \u001b[38;5;28mint\u001b[39m(proba[\u001b[32m1\u001b[39m] >= threshold)\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prediction, proba\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 16. FUNCI√ìN DE PREDICCI√ìN PARA STREAMLIT\n",
    "# ============================================================================\n",
    "\n",
    "def predict_hate_speech(text, model_artifacts):\n",
    "    \"\"\"\n",
    "    Predice si un texto es hate speech usando el modelo guardado.\n",
    "    \n",
    "    Args:\n",
    "        text: str - Texto a clasificar\n",
    "        model_artifacts: dict - Diccionario cargado desde el .pkl\n",
    "    \n",
    "    Returns:\n",
    "        prediction: int (0=Normal, 1=Hate)\n",
    "        probabilities: array [prob_normal, prob_hate]\n",
    "    \"\"\"\n",
    "    from scipy.sparse import hstack, csr_matrix\n",
    "    import re\n",
    "    \n",
    "    # Extraer componentes\n",
    "    model = model_artifacts['model']\n",
    "    vectorizer = model_artifacts['vectorizer']\n",
    "    scaler = model_artifacts['scaler']\n",
    "    scaler_advanced = model_artifacts.get('scaler_advanced')\n",
    "    threshold = model_artifacts.get('threshold', 0.5)\n",
    "    feature_columns = model_artifacts.get('feature_columns', [])\n",
    "    \n",
    "    # 1. TF-IDF\n",
    "    text_tfidf = vectorizer.transform([text])\n",
    "    \n",
    "    # 2. Features num√©ricas b√°sicas (rellenar con 0)\n",
    "    num_features = np.zeros((1, len(feature_columns)))\n",
    "    num_features_scaled = scaler.transform(num_features)\n",
    "    \n",
    "    # 3. Features avanzadas de texto\n",
    "    def extract_advanced_features(text):\n",
    "        offensive_words = [\n",
    "            'hate', 'stupid', 'idiot', 'dumb', 'kill', 'die', 'death',\n",
    "            'ugly', 'worst', 'terrible', 'awful', 'disgusting', 'pathetic',\n",
    "            'loser', 'trash', 'garbage', 'shit', 'fuck', 'damn',\n",
    "        ]\n",
    "        text_lower = text.lower()\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        features = {}\n",
    "        features['offensive_word_count'] = sum(1 for w in words if any(off in w for off in offensive_words))\n",
    "        features['offensive_word_ratio'] = features['offensive_word_count'] / len(words) if words else 0\n",
    "        features['avg_word_len'] = np.mean([len(w) for w in words]) if words else 0\n",
    "        features['max_word_len'] = max([len(w) for w in words]) if words else 0\n",
    "        features['char_repetition'] = len(re.findall(r'(.)\\1{2,}', text))\n",
    "        features['caps_words'] = sum(1 for w in words if w.isupper() and len(w) > 1)\n",
    "        features['multiple_punctuation'] = len(re.findall(r'[!?]{2,}', text))\n",
    "        negations = ['not', 'no', 'never', 'none', 'nobody', 'nothing', 'neither', 'nowhere', \"n't\"]\n",
    "        features['negation_count'] = sum(1 for w in words if w in negations)\n",
    "        pronouns = ['you', 'your', 'they', 'them', 'their', 'he', 'she', 'his', 'her']\n",
    "        features['pronoun_count'] = sum(1 for w in words if w in pronouns)\n",
    "        features['unique_word_ratio'] = len(set(words)) / len(words) if words else 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    adv_features = extract_advanced_features(text)\n",
    "    advanced_array = np.array([[\n",
    "        adv_features['offensive_word_count'],\n",
    "        adv_features['offensive_word_ratio'],\n",
    "        adv_features['avg_word_len'],\n",
    "        adv_features['max_word_len'],\n",
    "        adv_features['char_repetition'],\n",
    "        adv_features['caps_words'],\n",
    "        adv_features['multiple_punctuation'],\n",
    "        adv_features['negation_count'],\n",
    "        adv_features['pronoun_count'],\n",
    "        adv_features['unique_word_ratio'],\n",
    "    ]])\n",
    "    \n",
    "    if scaler_advanced is not None:\n",
    "        advanced_scaled = scaler_advanced.transform(advanced_array)\n",
    "    else:\n",
    "        advanced_scaled = advanced_array\n",
    "    \n",
    "    # 4. Combinar todas las features\n",
    "    X_combined = hstack([\n",
    "        text_tfidf,\n",
    "        csr_matrix(num_features_scaled),\n",
    "        csr_matrix(advanced_scaled)\n",
    "    ])\n",
    "    \n",
    "    # 5. Predicci√≥n\n",
    "    proba = model.predict_proba(X_combined)[0]\n",
    "    prediction = int(proba[1] >= threshold)\n",
    "    \n",
    "    return prediction, proba\n",
    "\n",
    "\n",
    "# TEST: Probar la funci√≥n\n",
    "if model_path.exists():\n",
    "    loaded_artifacts = joblib.load(model_path)\n",
    "    \n",
    "    test_texts = [\n",
    "        \"I love this video, great content!\",\n",
    "        \"You are so stupid and ugly, go die\",\n",
    "        \"This is interesting, thanks for sharing\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEST DE PREDICCI√ìN\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for text in test_texts:\n",
    "        pred, probs = predict_hate_speech(text, loaded_artifacts)\n",
    "        label = \"üî¥ HATE\" if pred == 1 else \"üü¢ Normal\"\n",
    "        print(f\"\\nTexto: '{text[:50]}...'\")\n",
    "        print(f\"  Predicci√≥n: {label}\")\n",
    "        print(f\"  Probabilidades: Normal={probs[0]:.3f}, Hate={probs[1]:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
