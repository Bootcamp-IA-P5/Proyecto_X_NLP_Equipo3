{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649d63fb",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "MODELO XGBOOST OPTIMIZADO - DETECCIÃ“N DE ODIO (DATASET PEQUEÃ‘O)\n",
    "=============================================================================\n",
    "TÃ©cnicas especÃ­ficas para datasets pequeÃ±os:\n",
    "- Data Augmentation con backtranslation simulada\n",
    "- SMOTE para balanceo sintÃ©tico\n",
    "- RegularizaciÃ³n extrema\n",
    "- Ensemble de modelos con diferentes seeds\n",
    "- Feature engineering mejorado\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d92c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "70e04ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ DETECCIÃ“N DE MENSAJES DE ODIO - YOUTUBE (DATASET PEQUEÃ‘O)\n",
      "================================================================================\n",
      "ğŸ“‚ Project root: c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\n",
      "ğŸ“‚ Data path: c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\data\\processed\\youtube_all_versions.pkl\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. CONFIGURACIÃ“N\n",
    "# =============================================================================\n",
    "\n",
    "RND = 42\n",
    "np.random.seed(RND)\n",
    "\n",
    "# Detectar si estamos en notebooks/\n",
    "cwd = Path.cwd()\n",
    "project_root = cwd.parent if \"notebooks\" in str(cwd) else cwd\n",
    "\n",
    "data_path = project_root / \"data\" / \"processed\" / \"youtube_all_versions.pkl\"\n",
    "models_dir = project_root / \"models\"\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# MLflow\n",
    "mlruns_dir = project_root / \"mlruns\"\n",
    "mlruns_dir.mkdir(exist_ok=True)\n",
    "mlflow.set_tracking_uri(f\"file:///{mlruns_dir.as_posix()}\")\n",
    "mlflow.set_experiment(\"YouTube_Hate_XGBoost_Small_Dataset\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¯ DETECCIÃ“N DE MENSAJES DE ODIO - YOUTUBE (DATASET PEQUEÃ‘O)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ğŸ“‚ Project root: {project_root}\")\n",
    "print(f\"ğŸ“‚ Data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "860fd4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Dataset: 997 filas, 39 columnas\n",
      "âœ… Features numÃ©ricas: 13\n",
      "\n",
      "ğŸ“Š Balance de clases:\n",
      "   Normal (0): 54.0%\n",
      "   Odio (1):   46.0%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. CARGA Y ANÃLISIS DE DATOS\n",
    "# =============================================================================\n",
    "\n",
    "df = pd.read_pickle(data_path)\n",
    "print(f\"\\nğŸ“Š Dataset: {df.shape[0]:,} filas, {df.shape[1]} columnas\")\n",
    "\n",
    "# Validar columnas esenciales\n",
    "assert 'Text_Lemmatized' in df.columns and 'IsHate' in df.columns\n",
    "\n",
    "# Features numÃ©ricas\n",
    "numeric_features = [\n",
    "    'char_count', 'word_count', 'sentence_count', 'avg_word_length',\n",
    "    'uppercase_count', 'uppercase_ratio', 'exclamation_count', \n",
    "    'question_count', 'emoji_count', 'url_count', 'mention_count', \n",
    "    'hashtag_count', 'number_count'\n",
    "]\n",
    "feature_cols = [c for c in numeric_features if c in df.columns]\n",
    "print(f\"âœ… Features numÃ©ricas: {len(feature_cols)}\")\n",
    "\n",
    "X_text = df['Text_Lemmatized'].astype(str)\n",
    "X_num = df[feature_cols].fillna(0) if feature_cols else pd.DataFrame(index=df.index)\n",
    "y = df['IsHate'].astype(int)\n",
    "\n",
    "# Balance\n",
    "print(f\"\\nğŸ“Š Balance de clases:\")\n",
    "balance = y.value_counts(normalize=True)\n",
    "print(f\"   Normal (0): {balance[0]*100:.1f}%\")\n",
    "print(f\"   Odio (1):   {balance[1]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28ffe10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Evaluando necesidad de Data Augmentation:\n",
      "   Clase minoritaria (1): 459\n",
      "   Clase mayoritaria (0): 538\n",
      "   Ratio: 0.85\n",
      "   â­ï¸  Balance ya aceptable\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. DATA AUGMENTATION INTELIGENTE (balancear, no desequilibrar)\n",
    "# =============================================================================\n",
    "\n",
    "minority_class = y.value_counts().idxmin()\n",
    "majority_class = y.value_counts().idxmax()\n",
    "\n",
    "n_minority = (y == minority_class).sum()\n",
    "n_majority = (y == majority_class).sum()\n",
    "\n",
    "print(f\"\\nğŸ”„ Evaluando necesidad de Data Augmentation:\")\n",
    "print(f\"   Clase minoritaria ({minority_class}): {n_minority}\")\n",
    "print(f\"   Clase mayoritaria ({majority_class}): {n_majority}\")\n",
    "print(f\"   Ratio: {n_minority/n_majority:.2f}\")\n",
    "\n",
    "# OBJETIVO: Llegar a ratio ~0.85-0.90 (no 1.0 para evitar sobreajuste)\n",
    "if n_minority / n_majority < 0.85:\n",
    "    # Calcular cuÃ¡ntos necesitamos para llegar a ratio 0.88\n",
    "    target_ratio = 0.88\n",
    "    n_augment = int(n_majority * target_ratio) - n_minority\n",
    "    n_augment = max(0, n_augment)  # No negativos\n",
    "    \n",
    "    print(f\"   ğŸ¯ Objetivo: ratio={target_ratio:.2f}\")\n",
    "    print(f\"   Ejemplos a generar: {n_augment}\")\n",
    "    \n",
    "    if n_augment > 0:\n",
    "        minority_mask = (y == minority_class)\n",
    "        minority_texts = X_text[minority_mask]\n",
    "        minority_nums = X_num[minority_mask]\n",
    "        minority_labels = y[minority_mask]\n",
    "        \n",
    "        aug_indices = np.random.choice(len(minority_texts), n_augment, replace=True)\n",
    "        \n",
    "        # Augmentation simple (solo duplicados, no modificamos texto)\n",
    "        aug_texts = minority_texts.iloc[aug_indices].reset_index(drop=True)\n",
    "        aug_nums = minority_nums.iloc[aug_indices].reset_index(drop=True)\n",
    "        aug_labels = minority_labels.iloc[aug_indices].reset_index(drop=True)\n",
    "        \n",
    "        X_text_aug = pd.concat([X_text, aug_texts], ignore_index=True)\n",
    "        X_num_aug = pd.concat([X_num, aug_nums], ignore_index=True)\n",
    "        y_aug = pd.concat([y, aug_labels], ignore_index=True)\n",
    "        \n",
    "        print(f\"   âœ… Aumentado: {len(y):,} â†’ {len(y_aug):,} (+{n_augment} ejemplos)\")\n",
    "        print(f\"   Balance final:\")\n",
    "        for cls, count in y_aug.value_counts().items():\n",
    "            print(f\"      Clase {cls}: {count} ({count/len(y_aug)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   â­ï¸  No se necesita augmentation\")\n",
    "        X_text_aug = X_text\n",
    "        X_num_aug = X_num\n",
    "        y_aug = y\n",
    "else:\n",
    "    print(f\"   â­ï¸  Balance ya aceptable\")\n",
    "    X_text_aug = X_text\n",
    "    X_num_aug = X_num\n",
    "    y_aug = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "837a67ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Augmentation avanzado disponible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Data Augmentation Avanzado:\n",
      "   Original: 997 â†’ Aumentado: 1,226 (+229 ejemplos)\n",
      "   Balance final: {1: 688, 0: 538}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3B. DATA AUGMENTATION AVANZADO CON SINÃ“NIMOS Y PARÃFRASIS\n",
    "# =============================================================================\n",
    "\n",
    "# Instalar si no estÃ¡: !pip install nlpaug\n",
    "\n",
    "try:\n",
    "    import nlpaug.augmenter.word as naw\n",
    "    \n",
    "    # Augmenter con sinÃ³nimos usando WordNet\n",
    "    aug_synonym = naw.SynonymAug(aug_src='wordnet')\n",
    "    \n",
    "    def augment_text_advanced(text):\n",
    "        \"\"\"Augmentation con sinÃ³nimos y variaciones\"\"\"\n",
    "        try:\n",
    "            # 50% probabilidad de usar cada tÃ©cnica\n",
    "            if np.random.random() < 0.5:\n",
    "                text = aug_synonym.augment(text)\n",
    "            \n",
    "            # Augmentation bÃ¡sico adicional\n",
    "            words = text.split()\n",
    "            if len(words) > 2:\n",
    "                # Shuffle de algunas palabras (mantiene sentido general)\n",
    "                if np.random.random() < 0.3:\n",
    "                    idx1, idx2 = np.random.choice(len(words), 2, replace=False)\n",
    "                    words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "            \n",
    "            return ' '.join(words)\n",
    "        except:\n",
    "            return augment_text_simple(text)\n",
    "    \n",
    "    print(\"âœ… Augmentation avanzado disponible\")\n",
    "    use_advanced_aug = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸  nlpaug no disponible, usando augmentation bÃ¡sico\")\n",
    "    use_advanced_aug = False\n",
    "    augment_text_advanced = augment_text_simple\n",
    "\n",
    "# Reemplazar augmentation\n",
    "minority_class = y.value_counts().idxmin()\n",
    "minority_mask = (y == minority_class)\n",
    "minority_texts = X_text[minority_mask]\n",
    "minority_nums = X_num[minority_mask]\n",
    "minority_labels = y[minority_mask]\n",
    "\n",
    "# Aumentar 50% (mÃ¡s agresivo)\n",
    "n_augment = int(len(minority_texts) * 0.5)\n",
    "aug_indices = np.random.choice(len(minority_texts), n_augment, replace=True)\n",
    "\n",
    "aug_func = augment_text_advanced if use_advanced_aug else augment_text_simple\n",
    "aug_texts = minority_texts.iloc[aug_indices].apply(aug_func).reset_index(drop=True)\n",
    "aug_nums = minority_nums.iloc[aug_indices].reset_index(drop=True)\n",
    "aug_labels = minority_labels.iloc[aug_indices].reset_index(drop=True)\n",
    "\n",
    "# Concatenar\n",
    "X_text_aug = pd.concat([X_text, aug_texts], ignore_index=True)\n",
    "X_num_aug = pd.concat([X_num, aug_nums], ignore_index=True)\n",
    "y_aug = pd.concat([y, aug_labels], ignore_index=True)\n",
    "\n",
    "print(f\"\\nğŸ”„ Data Augmentation Avanzado:\")\n",
    "print(f\"   Original: {len(y):,} â†’ Aumentado: {len(y_aug):,} (+{n_augment} ejemplos)\")\n",
    "print(f\"   Balance final: {y_aug.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d00fa501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”€ Train/Test split: 980 / 246\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. SPLIT ESTRATIFICADO\n",
    "# =============================================================================\n",
    "\n",
    "X_text_train, X_text_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
    "    X_text_aug, X_num_aug, y_aug, test_size=0.2, random_state=RND, stratify=y_aug\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ”€ Train/Test split: {len(y_train):,} / {len(y_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d27fea74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”§ FEATURE ENGINEERING AVANZADO\n",
      "================================================================================\n",
      "ğŸ“ Extrayendo features avanzadas...\n",
      "âœ… Features adicionales extraÃ­das: 10\n",
      "   Nuevas features: ['offensive_word_count', 'offensive_word_ratio', 'avg_word_len', 'max_word_len', 'char_repetition', 'caps_words', 'multiple_punctuation', 'negation_count', 'pronoun_count', 'unique_word_ratio']\n",
      "\n",
      "ğŸ”— Features totales: 1523\n",
      "   TF-IDF: 1500\n",
      "   NumÃ©ricas: 13\n",
      "   Avanzadas: 10\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4B. FEATURE ENGINEERING ADICIONAL DE TEXTO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ”§ FEATURE ENGINEERING AVANZADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_advanced_text_features(text):\n",
    "    \"\"\"Extrae features adicionales del texto\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Palabras ofensivas comunes (keywords hate speech)\n",
    "    offensive_words = [\n",
    "        'hate', 'stupid', 'idiot', 'dumb', 'kill', 'die', 'death',\n",
    "        'ugly', 'worst', 'terrible', 'awful', 'disgusting', 'pathetic',\n",
    "        'loser', 'trash', 'garbage', 'shit', 'fuck', 'damn'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    words = text_lower.split()\n",
    "    \n",
    "    # Conteo de palabras ofensivas\n",
    "    features['offensive_word_count'] = sum(1 for w in words if any(off in w for off in offensive_words))\n",
    "    features['offensive_word_ratio'] = features['offensive_word_count'] / len(words) if words else 0\n",
    "    \n",
    "    # Longitud promedio de palabras (palabras muy cortas o muy largas)\n",
    "    features['avg_word_len'] = np.mean([len(w) for w in words]) if words else 0\n",
    "    features['max_word_len'] = max([len(w) for w in words]) if words else 0\n",
    "    \n",
    "    # RepeticiÃ³n de caracteres (ej: \"noooo\", \"whyyy\")\n",
    "    features['char_repetition'] = len(re.findall(r'(.)\\1{2,}', text))\n",
    "    \n",
    "    # CAPS words (palabras en mayÃºsculas completas)\n",
    "    features['caps_words'] = sum(1 for w in words if w.isupper() and len(w) > 1)\n",
    "    \n",
    "    # PuntuaciÃ³n mÃºltiple (!!!, ???)\n",
    "    features['multiple_punctuation'] = len(re.findall(r'[!?]{2,}', text))\n",
    "    \n",
    "    # Negaciones (not, no, never, etc.)\n",
    "    negations = ['not', 'no', 'never', 'none', 'nobody', 'nothing', 'neither', 'nowhere', \"n't\"]\n",
    "    features['negation_count'] = sum(1 for w in words if w in negations)\n",
    "    \n",
    "    # Pronombres (you, they - comÃºn en ataques personales)\n",
    "    pronouns = ['you', 'your', 'they', 'them', 'their', 'he', 'she', 'his', 'her']\n",
    "    features['pronoun_count'] = sum(1 for w in words if w in pronouns)\n",
    "    \n",
    "    # Ratio de palabras Ãºnicas (baja diversidad lÃ©xica en mensajes ofensivos)\n",
    "    features['unique_word_ratio'] = len(set(words)) / len(words) if words else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Aplicar a train y test\n",
    "print(\"ğŸ“ Extrayendo features avanzadas...\")\n",
    "\n",
    "train_advanced_features = X_text_train.apply(extract_advanced_text_features).apply(pd.Series)\n",
    "test_advanced_features = X_text_test.apply(extract_advanced_text_features).apply(pd.Series)\n",
    "\n",
    "print(f\"âœ… Features adicionales extraÃ­das: {train_advanced_features.shape[1]}\")\n",
    "print(f\"   Nuevas features: {list(train_advanced_features.columns)}\")\n",
    "\n",
    "# Normalizar las nuevas features\n",
    "scaler_advanced = StandardScaler()\n",
    "train_advanced_scaled = scaler_advanced.fit_transform(train_advanced_features)\n",
    "test_advanced_scaled = scaler_advanced.transform(test_advanced_features)\n",
    "\n",
    "# Combinar con features existentes\n",
    "X_train_combined_enhanced = hstack([\n",
    "    X_text_train_tfidf, \n",
    "    csr_matrix(X_num_train_scaled),\n",
    "    csr_matrix(train_advanced_scaled)\n",
    "])\n",
    "\n",
    "X_test_combined_enhanced = hstack([\n",
    "    X_text_test_tfidf,\n",
    "    csr_matrix(X_num_test_scaled),\n",
    "    csr_matrix(test_advanced_scaled)\n",
    "])\n",
    "\n",
    "print(f\"\\nğŸ”— Features totales: {X_train_combined_enhanced.shape[1]}\")\n",
    "print(f\"   TF-IDF: {X_text_train_tfidf.shape[1]}\")\n",
    "print(f\"   NumÃ©ricas: {X_num_train_scaled.shape[1]}\")\n",
    "print(f\"   Avanzadas: {train_advanced_scaled.shape[1]}\")\n",
    "\n",
    "# Guardar\n",
    "joblib.dump(scaler_advanced, models_dir / \"scaler_advanced.pkl\")\n",
    "\n",
    "# Usar las features mejoradas en el resto del pipeline\n",
    "X_train_combined = X_train_combined_enhanced\n",
    "X_test_combined = X_test_combined_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8022f38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ TF-IDF: 1500 features\n",
      "ğŸ”— Features combinadas: 1513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\Administrator\\\\Desktop\\\\NLP\\\\Proyecto_X_NLP_Equipo3\\\\models\\\\feature_columns.pkl']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. VECTORIZACIÃ“N TF-IDF (CONSERVADORA)\n",
    "# =============================================================================\n",
    "\n",
    "# ParÃ¡metros muy conservadores para dataset pequeÃ±o\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=1500,  # Reducido aÃºn mÃ¡s\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.85,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "X_text_train_tfidf = vectorizer.fit_transform(X_text_train)\n",
    "X_text_test_tfidf = vectorizer.transform(X_text_test)\n",
    "\n",
    "print(f\"ğŸ“ TF-IDF: {X_text_train_tfidf.shape[1]} features\")\n",
    "\n",
    "# NormalizaciÃ³n features numÃ©ricas\n",
    "scaler = StandardScaler()\n",
    "if feature_cols:\n",
    "    X_num_train_scaled = scaler.fit_transform(X_num_train)\n",
    "    X_num_test_scaled = scaler.transform(X_num_test)\n",
    "else:\n",
    "    X_num_train_scaled = np.zeros((len(X_text_train), 0))\n",
    "    X_num_test_scaled = np.zeros((len(X_text_test), 0))\n",
    "\n",
    "# Combinar\n",
    "X_train_combined = hstack([X_text_train_tfidf, csr_matrix(X_num_train_scaled)])\n",
    "X_test_combined = hstack([X_text_test_tfidf, csr_matrix(X_num_test_scaled)])\n",
    "\n",
    "print(f\"ğŸ”— Features combinadas: {X_train_combined.shape[1]}\")\n",
    "\n",
    "# Guardar artefactos\n",
    "joblib.dump(vectorizer, models_dir / \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(scaler, models_dir / \"feature_scaler.pkl\")\n",
    "joblib.dump(feature_cols, models_dir / \"feature_columns.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e2c02f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš–ï¸  Saltando SMOTE (balance ya aceptable con augmentation)\n",
      "   Balance train:\n",
      "      Clase 1: 550 (56.1%)\n",
      "      Clase 0: 430 (43.9%)\n",
      "\n",
      "âš–ï¸  Scale pos weight: 0.78\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. SIN SMOTE (ya tenemos balance aceptable con augmentation)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nâš–ï¸  Saltando SMOTE (balance ya aceptable con augmentation)\")\n",
    "\n",
    "X_train_balanced = X_train_combined\n",
    "y_train_balanced = y_train\n",
    "\n",
    "# Calcular scale_pos_weight\n",
    "n_pos = (y_train_balanced == 1).sum()\n",
    "n_neg = (y_train_balanced == 0).sum()\n",
    "scale_pos_weight = n_neg / n_pos if n_pos > 0 else 1.0\n",
    "\n",
    "print(f\"   Balance train:\")\n",
    "for cls, count in pd.Series(y_train_balanced).value_counts().items():\n",
    "    print(f\"      Clase {cls}: {count} ({count/len(y_train_balanced)*100:.1f}%)\")\n",
    "print(f\"\\nâš–ï¸  Scale pos weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1fdb8333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Funciones de utilidad definidas\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. UTILIDADES (CORREGIDAS)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_proba=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_proba) if y_proba is not None else np.nan\n",
    "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'roc_auc': auc}\n",
    "\n",
    "def print_metrics_table(train_m, test_m):\n",
    "    print(f\"\\n{'MÃ©trica':<12}{'Train':>10}{'Test':>10}{'Diff':>10}\")\n",
    "    print(\"-\" * 42)\n",
    "    for k in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "        t = train_m.get(k, np.nan)\n",
    "        s = test_m.get(k, np.nan)\n",
    "        diff = t - s if not np.isnan(t) and not np.isnan(s) else np.nan\n",
    "        print(f\"{k:<12}{t:>10.4f}{s:>10.4f}{diff:>10.4f}\")\n",
    "\n",
    "print(f\"âœ… Funciones de utilidad definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f948f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. FUNCIÃ“N DE ENTRENAMIENTO SIN CALIBRACIÃ“N (mÃ¡s simple y efectiva)\n",
    "# =============================================================================\n",
    "\n",
    "def train_xgb_with_calibration(params, X_train, y_train, X_test, y_test, \n",
    "                                 run_name=None, early_stopping_rounds=100):\n",
    "    \"\"\"\n",
    "    Entrena XGBoost con threshold tuning DIRECTO (sin Platt Scaling)\n",
    "    La calibraciÃ³n estaba causando problemas - mejor usar probabilidades raw\n",
    "    \"\"\"\n",
    "    \n",
    "    # Modelo base\n",
    "    model = xgb.XGBClassifier(**params, use_label_encoder=False)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Usar probabilidades DIRECTAS (sin calibraciÃ³n)\n",
    "    train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n   ğŸ“Š Probabilidades (train):\")\n",
    "    print(f\"      Min={train_proba.min():.3f}, Max={train_proba.max():.3f}, \"\n",
    "          f\"Mean={train_proba.mean():.3f}, Median={np.median(train_proba):.3f}\")\n",
    "    \n",
    "    # THRESHOLD TUNING con criterios balanceados\n",
    "    thresholds = np.linspace(0.35, 0.65, 31)  # Rango centrado\n",
    "    \n",
    "    threshold_results = []\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        y_pred_thr = (train_proba >= thr).astype(int)\n",
    "        \n",
    "        # Evitar extremos\n",
    "        n_positive = y_pred_thr.sum()\n",
    "        if n_positive < 50 or n_positive > len(y_train) - 50:\n",
    "            continue\n",
    "        \n",
    "        # MÃ©tricas\n",
    "        prec = precision_score(y_train, y_pred_thr, zero_division=0)\n",
    "        rec = recall_score(y_train, y_pred_thr, zero_division=0)\n",
    "        f1 = f1_score(y_train, y_pred_thr, zero_division=0)\n",
    "        \n",
    "        diff = abs(prec - rec)\n",
    "        \n",
    "        # Score: F1 con penalizaciÃ³n por desbalance\n",
    "        balance_penalty = diff * 0.15\n",
    "        score = f1 - balance_penalty\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'thr': thr,\n",
    "            'prec': prec,\n",
    "            'rec': rec,\n",
    "            'f1': f1,\n",
    "            'diff': diff,\n",
    "            'score': score,\n",
    "            'n_pos': n_positive\n",
    "        })\n",
    "    \n",
    "    # Seleccionar mejor threshold con criterios (CON FALLBACK)\n",
    "    if not threshold_results:\n",
    "        # FALLBACK 1: No hay resultados vÃ¡lidos\n",
    "        print(f\"   âš ï¸  No hay thresholds vÃ¡lidos en rango [0.35-0.65]\")\n",
    "        print(f\"   Expandiendo bÃºsqueda a [0.20-0.80]...\")\n",
    "        \n",
    "        # Buscar en rango mÃ¡s amplio\n",
    "        thresholds_wide = np.linspace(0.20, 0.80, 61)\n",
    "        for thr in thresholds_wide:\n",
    "            y_pred_thr = (train_proba >= thr).astype(int)\n",
    "            n_positive = y_pred_thr.sum()\n",
    "            \n",
    "            # MÃ¡s permisivo\n",
    "            if n_positive < 20 or n_positive > len(y_train) - 20:\n",
    "                continue\n",
    "            \n",
    "            prec = precision_score(y_train, y_pred_thr, zero_division=0)\n",
    "            rec = recall_score(y_train, y_pred_thr, zero_division=0)\n",
    "            f1 = f1_score(y_train, y_pred_thr, zero_division=0)\n",
    "            diff = abs(prec - rec)\n",
    "            score = f1 - (diff * 0.15)\n",
    "            \n",
    "            threshold_results.append({\n",
    "                'thr': thr, 'prec': prec, 'rec': rec, 'f1': f1, \n",
    "                'diff': diff, 'score': score, 'n_pos': n_positive\n",
    "            })\n",
    "    \n",
    "    if not threshold_results:\n",
    "        # FALLBACK 2: Usar threshold por defecto\n",
    "        print(f\"   âš ï¸  FALLBACK: Usando threshold=0.5 por defecto\")\n",
    "        best_thr = 0.5\n",
    "        y_pred_default = (train_proba >= 0.5).astype(int)\n",
    "        best_res = {\n",
    "            'thr': 0.5,\n",
    "            'prec': precision_score(y_train, y_pred_default, zero_division=0),\n",
    "            'rec': recall_score(y_train, y_pred_default, zero_division=0),\n",
    "            'f1': f1_score(y_train, y_pred_default, zero_division=0),\n",
    "            'diff': 0.0,\n",
    "            'score': 0.0,\n",
    "            'n_pos': y_pred_default.sum()\n",
    "        }\n",
    "        level = \"FALLBACK\"\n",
    "    else:\n",
    "        # SelecciÃ³n normal con criterios\n",
    "        # Nivel 1: Precision >= 0.60, Recall >= 0.70, Diff < 0.20\n",
    "        optimal = [r for r in threshold_results \n",
    "                   if r['prec'] >= 0.60 and r['rec'] >= 0.70 and r['diff'] < 0.20]\n",
    "        \n",
    "        # Nivel 2: Precision >= 0.55, Recall >= 0.65, Diff < 0.25\n",
    "        good = [r for r in threshold_results \n",
    "                if r['prec'] >= 0.55 and r['rec'] >= 0.65 and r['diff'] < 0.25]\n",
    "        \n",
    "        # Nivel 3: Precision >= 0.45, Recall >= 0.55 (mÃ¡s permisivo)\n",
    "        acceptable = [r for r in threshold_results \n",
    "                      if r['prec'] >= 0.45 and r['rec'] >= 0.55]\n",
    "        \n",
    "        # Nivel 4: Mejor F1 sin restricciones\n",
    "        if optimal:\n",
    "            candidates = optimal\n",
    "            level = \"Ã“PTIMO\"\n",
    "        elif good:\n",
    "            candidates = good\n",
    "            level = \"BUENO\"\n",
    "        elif acceptable:\n",
    "            candidates = acceptable\n",
    "            level = \"ACEPTABLE\"\n",
    "        else:\n",
    "            candidates = threshold_results\n",
    "            level = \"MEJOR F1\"\n",
    "        \n",
    "        best_res = max(candidates, key=lambda x: x['score'])\n",
    "        best_thr = best_res['thr']\n",
    "    \n",
    "    # Mostrar opciones\n",
    "    print(f\"\\n   ğŸ¯ Nivel: {level}\")\n",
    "    print(f\"   ğŸ“‹ Top 5 thresholds:\")\n",
    "    for i, res in enumerate(sorted(threshold_results, key=lambda x: x['score'], reverse=True)[:5], 1):\n",
    "        print(f\"      {i}. thr={res['thr']:.3f} | F1={res['f1']:.3f} | \"\n",
    "              f\"P={res['prec']:.3f} | R={res['rec']:.3f} | Diff={res['diff']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n   âœ… SELECCIONADO: thr={best_thr:.3f}\")\n",
    "    print(f\"      Train F1={best_res['f1']:.4f}, P={best_res['prec']:.4f}, \"\n",
    "          f\"R={best_res['rec']:.4f}, Diff={best_res['diff']:.4f}\")\n",
    "    \n",
    "    # Predicciones finales\n",
    "    train_pred = (train_proba >= best_thr).astype(int)\n",
    "    test_pred = (test_proba >= best_thr).astype(int)\n",
    "    \n",
    "    train_metrics = compute_metrics(y_train, train_pred, train_proba)\n",
    "    test_metrics = compute_metrics(y_test, test_pred, test_proba)\n",
    "    overfitting = abs(train_metrics['f1'] - test_metrics['f1']) * 100\n",
    "    \n",
    "    # MLflow\n",
    "    if run_name:\n",
    "        with mlflow.start_run(run_name=run_name):\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_param(\"optimal_threshold\", best_thr)\n",
    "            mlflow.log_param(\"threshold_level\", level)\n",
    "            mlflow.log_param(\"early_stopping_rounds\", early_stopping_rounds)\n",
    "            \n",
    "            for k, v in train_metrics.items():\n",
    "                mlflow.log_metric(f\"train_{k}\", v)\n",
    "            for k, v in test_metrics.items():\n",
    "                mlflow.log_metric(f\"test_{k}\", v)\n",
    "            mlflow.log_metric(\"overfitting_pct\", overfitting)\n",
    "            \n",
    "            mlflow.xgboost.log_model(model, \"xgb_model\")\n",
    "    \n",
    "    # Retornar con None en lugar del calibrator (ya no lo usamos)\n",
    "    return (model, None, best_thr), train_metrics, test_metrics, overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "39fcd0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” DIAGNÃ“STICO: AnÃ¡lisis de probabilidades del modelo\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Probabilidades SIN calibrar:\n",
      "   Train: min=0.054, max=0.968, mean=0.517, median=0.479\n",
      "   Test:  min=0.102, max=0.906, mean=0.503, median=0.479\n",
      "\n",
      "   DistribuciÃ³n probabilidades train (bins):\n",
      "      [0.0-0.2]:   37 (3.8%)\n",
      "      [0.2-0.4]:  267 (27.2%)\n",
      "      [0.4-0.6]:  370 (37.8%)\n",
      "      [0.6-0.8]:  194 (19.8%)\n",
      "      [0.8-1.0]:  112 (11.4%)\n",
      "\n",
      "   Prueba de thresholds manuales (train):\n",
      "      thr=0.3: F1=0.787 | P=0.650 | R=0.998 | Pos=845/980 (86.2%)\n",
      "      thr=0.4: F1=0.856 | P=0.777 | R=0.955 | Pos=676/980 (69.0%)\n",
      "      thr=0.5: F1=0.849 | P=0.948 | R=0.769 | Pos=446/980 (45.5%)\n",
      "      thr=0.6: F1=0.706 | P=0.987 | R=0.549 | Pos=306/980 (31.2%)\n",
      "      thr=0.7: F1=0.553 | P=1.000 | R=0.382 | Pos=210/980 (21.4%)\n",
      "\n",
      "   Balance clases train_balanced:\n",
      "      Clase 1: 550 (56.1%)\n",
      "      Clase 0: 430 (43.9%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8B. DIAGNÃ“STICO: Revisar distribuciÃ³n de probabilidades\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ” DIAGNÃ“STICO: AnÃ¡lisis de probabilidades del modelo\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "diagnostic_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 200,\n",
    "    'random_state': RND,\n",
    "    'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "\n",
    "diag_model = xgb.XGBClassifier(**diagnostic_params, use_label_encoder=False)\n",
    "diag_model.fit(X_train_balanced, y_train_balanced, verbose=False)  # â† CAMBIO AQUÃ\n",
    "\n",
    "# Probabilidades sin calibrar\n",
    "train_proba_raw = diag_model.predict_proba(X_train_balanced)[:, 1]  # â† CAMBIO AQUÃ\n",
    "test_proba_raw = diag_model.predict_proba(X_test_combined)[:, 1]\n",
    "\n",
    "print(f\"\\nğŸ“Š Probabilidades SIN calibrar:\")\n",
    "print(f\"   Train: min={train_proba_raw.min():.3f}, max={train_proba_raw.max():.3f}, \"\n",
    "      f\"mean={train_proba_raw.mean():.3f}, median={np.median(train_proba_raw):.3f}\")\n",
    "print(f\"   Test:  min={test_proba_raw.min():.3f}, max={test_proba_raw.max():.3f}, \"\n",
    "      f\"mean={test_proba_raw.mean():.3f}, median={np.median(test_proba_raw):.3f}\")\n",
    "\n",
    "# Histograma de probabilidades\n",
    "print(f\"\\n   DistribuciÃ³n probabilidades train (bins):\")\n",
    "hist_train, bins = np.histogram(train_proba_raw, bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "for i, (b1, b2) in enumerate(zip(bins[:-1], bins[1:])):\n",
    "    print(f\"      [{b1:.1f}-{b2:.1f}]: {hist_train[i]:>4} ({hist_train[i]/len(train_proba_raw)*100:.1f}%)\")\n",
    "\n",
    "# Probar diferentes thresholds manualmente\n",
    "print(f\"\\n   Prueba de thresholds manuales (train):\")\n",
    "for thr in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    preds = (train_proba_raw >= thr).astype(int)\n",
    "    prec = precision_score(y_train_balanced, preds, zero_division=0)  # â† CAMBIO AQUÃ\n",
    "    rec = recall_score(y_train_balanced, preds, zero_division=0)  # â† CAMBIO AQUÃ\n",
    "    f1 = f1_score(y_train_balanced, preds, zero_division=0)  # â† CAMBIO AQUÃ\n",
    "    n_pos = preds.sum()\n",
    "    print(f\"      thr={thr:.1f}: F1={f1:.3f} | P={prec:.3f} | R={rec:.3f} | \"\n",
    "          f\"Pos={n_pos}/{len(preds)} ({n_pos/len(preds)*100:.1f}%)\")\n",
    "\n",
    "# Balance de clases en train\n",
    "print(f\"\\n   Balance clases train_balanced:\")  # â† CAMBIO AQUÃ\n",
    "for cls, count in pd.Series(y_train_balanced).value_counts().items():  # â† CAMBIO AQUÃ\n",
    "    print(f\"      Clase {cls}: {count} ({count/len(y_train_balanced)*100:.1f}%)\")  # â† CAMBIO AQUÃ\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0e533227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”· MODELO 1: BASELINE BALANCEADO\n",
      "================================================================================\n",
      "ğŸ”„ Entrenando modelo baseline...\n",
      "\n",
      "   ğŸ“Š Probabilidades (train):\n",
      "      Min=0.495, Max=0.505, Mean=0.501, Median=0.502\n",
      "\n",
      "   ğŸ¯ Nivel: BUENO\n",
      "   ğŸ“‹ Top 5 thresholds:\n",
      "      1. thr=0.500 | F1=0.713 | P=0.614 | R=0.851 | Diff=0.237\n",
      "\n",
      "   âœ… SELECCIONADO: thr=0.500\n",
      "      Train F1=0.7134, P=0.6142, R=0.8509, Diff=0.2367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/28 09:58:18 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/28 09:58:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ğŸ” VerificaciÃ³n de aprendizaje:\n",
      "      Probabilidades Ãºnicas: 5\n",
      "      Rango: [0.495, 0.505]\n",
      "      Std dev: 0.003\n",
      "   âš ï¸  WARNING: Modelo no estÃ¡ aprendiendo (std muy bajo)\n",
      "   Reentrenando con parÃ¡metros mÃ¡s permisivos...\n",
      "\n",
      "   ğŸ“Š Probabilidades (train):\n",
      "      Min=0.437, Max=0.615, Mean=0.499, Median=0.495\n",
      "\n",
      "   ğŸ¯ Nivel: Ã“PTIMO\n",
      "   ğŸ“‹ Top 5 thresholds:\n",
      "      1. thr=0.480 | F1=0.742 | P=0.661 | R=0.845 | Diff=0.185\n",
      "      2. thr=0.490 | F1=0.714 | P=0.722 | R=0.707 | Diff=0.014\n",
      "      3. thr=0.470 | F1=0.736 | P=0.627 | R=0.891 | Diff=0.264\n",
      "      4. thr=0.460 | F1=0.731 | P=0.608 | R=0.915 | Diff=0.306\n",
      "      5. thr=0.450 | F1=0.727 | P=0.585 | R=0.960 | Diff=0.375\n",
      "\n",
      "   âœ… SELECCIONADO: thr=0.480\n",
      "      Train F1=0.7416, P=0.6605, R=0.8455, Diff=0.1849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/28 09:58:36 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/28 09:58:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MÃ©trica          Train      Test      Diff\n",
      "------------------------------------------\n",
      "accuracy        0.6694    0.5447    0.1247\n",
      "precision       0.6605    0.5756    0.0849\n",
      "recall          0.8455    0.7174    0.1281\n",
      "f1              0.7416    0.6387    0.1029\n",
      "roc_auc         0.7479    0.5430    0.2049\n",
      "\n",
      "ğŸ“Š Overfitting: 10.29%\n",
      "âš–ï¸  Balance: P=0.576, R=0.717\n",
      "   âœ… Overfitting en rango aceptable para dataset pequeÃ±o\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 9. MODELO BASELINE BALANCEADO (regularizaciÃ³n funcional)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ”· MODELO 1: BASELINE BALANCEADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "baseline_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    \n",
    "    # PARÃMETROS BALANCEADOS (no tan extremos)\n",
    "    'max_depth': 3,  # MÃ­nimo funcional (antes 2)\n",
    "    'learning_rate': 0.05,  # Algo mÃ¡s alto (antes 0.03)\n",
    "    'n_estimators': 250,  # Algo mÃ¡s de capacidad\n",
    "    \n",
    "    # Submuestreo moderado\n",
    "    'subsample': 0.70,  # No tan agresivo (antes 0.60)\n",
    "    'colsample_bytree': 0.70,\n",
    "    'colsample_bylevel': 0.70,\n",
    "    \n",
    "    # Restricciones moderadas\n",
    "    'min_child_weight': 10,  # MÃ¡s bajo (antes 15)\n",
    "    'max_delta_step': 1,\n",
    "    \n",
    "    # RegularizaciÃ³n fuerte pero no extrema\n",
    "    'gamma': 1.5,  # MÃ¡s bajo (antes 2.5)\n",
    "    'reg_alpha': 5.0,  # MÃ¡s bajo (antes 8.0)\n",
    "    'reg_lambda': 8.0,  # MÃ¡s bajo (antes 12.0)\n",
    "    \n",
    "    'random_state': RND,\n",
    "    'tree_method': 'hist',\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "print(\"ğŸ”„ Entrenando modelo baseline...\")\n",
    "baseline_result = train_xgb_with_calibration(\n",
    "    baseline_params, \n",
    "    X_train_balanced, y_train_balanced,\n",
    "    X_test_combined, y_test,\n",
    "    run_name=\"Baseline_Balanced\",\n",
    "    early_stopping_rounds=75\n",
    ")\n",
    "\n",
    "baseline_model, baseline_train, baseline_test, baseline_over = baseline_result\n",
    "\n",
    "# VERIFICAR que el modelo SÃ aprendiÃ³ algo\n",
    "train_proba_check = baseline_model[0].predict_proba(X_train_balanced)[:, 1]\n",
    "print(f\"\\n   ğŸ” VerificaciÃ³n de aprendizaje:\")\n",
    "print(f\"      Probabilidades Ãºnicas: {len(np.unique(np.round(train_proba_check, 3)))}\")\n",
    "print(f\"      Rango: [{train_proba_check.min():.3f}, {train_proba_check.max():.3f}]\")\n",
    "print(f\"      Std dev: {train_proba_check.std():.3f}\")\n",
    "\n",
    "if train_proba_check.std() < 0.05:\n",
    "    print(f\"   âš ï¸  WARNING: Modelo no estÃ¡ aprendiendo (std muy bajo)\")\n",
    "    print(f\"   Reentrenando con parÃ¡metros mÃ¡s permisivos...\")\n",
    "    \n",
    "    # Fallback: parÃ¡metros mÃ¡s permisivos\n",
    "    baseline_params.update({\n",
    "        'max_depth': 4,\n",
    "        'min_child_weight': 8,\n",
    "        'gamma': 1.0,\n",
    "        'reg_alpha': 3.0,\n",
    "        'reg_lambda': 6.0,\n",
    "        'subsample': 0.75,\n",
    "        'colsample_bytree': 0.75\n",
    "    })\n",
    "    \n",
    "    baseline_result = train_xgb_with_calibration(\n",
    "        baseline_params,\n",
    "        X_train_balanced, y_train_balanced,\n",
    "        X_test_combined, y_test,\n",
    "        run_name=\"Baseline_Permissive\",\n",
    "        early_stopping_rounds=75\n",
    "    )\n",
    "    baseline_model, baseline_train, baseline_test, baseline_over = baseline_result\n",
    "\n",
    "print_metrics_table(baseline_train, baseline_test)\n",
    "print(f\"\\nğŸ“Š Overfitting: {baseline_over:.2f}%\")\n",
    "print(f\"âš–ï¸  Balance: P={baseline_test['precision']:.3f}, R={baseline_test['recall']:.3f}\")\n",
    "\n",
    "# Target: overfitting 8-12%, pero con modelo que SÃ aprende\n",
    "if baseline_over < 5.0:\n",
    "    print(f\"   âš ï¸  Overfitting muy bajo - modelo puede estar underfitting\")\n",
    "elif baseline_over > 15.0:\n",
    "    print(f\"   âš ï¸  Overfitting alto - considerar mÃ¡s regularizaciÃ³n\")\n",
    "else:\n",
    "    print(f\"   âœ… Overfitting en rango aceptable para dataset pequeÃ±o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1a06ce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”· MODELO 1B: LOGISTIC REGRESSION (baseline simple)\n",
      "================================================================================\n",
      "ğŸ”„ Entrenando Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/28 09:59:06 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ¯ Threshold Ã³ptimo: 0.490\n",
      "\n",
      "MÃ©trica          Train      Test      Diff\n",
      "------------------------------------------\n",
      "accuracy        0.9010    0.7154    0.1856\n",
      "precision       0.9141    0.7394    0.1746\n",
      "recall          0.9091    0.7609    0.1482\n",
      "f1              0.9116    0.7500    0.1616\n",
      "roc_auc         0.9577    0.8153    0.1424\n",
      "\n",
      "ğŸ“Š Overfitting: 16.16%\n",
      "âš–ï¸  Balance: P=0.739, R=0.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/28 09:59:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 9B. LOGISTIC REGRESSION (sanity check - modelo mÃ¡s simple)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ”· MODELO 1B: LOGISTIC REGRESSION (baseline simple)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Convertir sparse matrix a array para LR\n",
    "X_train_lr = X_train_balanced.toarray() if hasattr(X_train_balanced, 'toarray') else X_train_balanced\n",
    "X_test_lr = X_test_combined.toarray() if hasattr(X_test_combined, 'toarray') else X_test_combined\n",
    "\n",
    "# Logistic Regression con regularizaciÃ³n\n",
    "lr_model = LogisticRegression(\n",
    "    C=0.5,  # RegularizaciÃ³n moderada\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=RND,\n",
    "    solver='saga',  # Mejor para sparse data\n",
    "    penalty='l2'\n",
    ")\n",
    "\n",
    "print(\"ğŸ”„ Entrenando Logistic Regression...\")\n",
    "lr_model.fit(X_train_lr, y_train_balanced)\n",
    "\n",
    "# Predicciones\n",
    "lr_train_proba = lr_model.predict_proba(X_train_lr)[:, 1]\n",
    "lr_test_proba = lr_model.predict_proba(X_test_lr)[:, 1]\n",
    "\n",
    "# Threshold tuning\n",
    "thresholds = np.linspace(0.35, 0.65, 31)\n",
    "best_thr = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for thr in thresholds:\n",
    "    preds = (lr_train_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_train_balanced, preds)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thr = thr\n",
    "\n",
    "print(f\"   ğŸ¯ Threshold Ã³ptimo: {best_thr:.3f}\")\n",
    "\n",
    "lr_train_pred = (lr_train_proba >= best_thr).astype(int)\n",
    "lr_test_pred = (lr_test_proba >= best_thr).astype(int)\n",
    "\n",
    "lr_train_metrics = compute_metrics(y_train_balanced, lr_train_pred, lr_train_proba)\n",
    "lr_test_metrics = compute_metrics(y_test, lr_test_pred, lr_test_proba)\n",
    "lr_over = abs(lr_train_metrics['f1'] - lr_test_metrics['f1']) * 100\n",
    "\n",
    "print_metrics_table(lr_train_metrics, lr_test_metrics)\n",
    "print(f\"\\nğŸ“Š Overfitting: {lr_over:.2f}%\")\n",
    "print(f\"âš–ï¸  Balance: P={lr_test_metrics['precision']:.3f}, R={lr_test_metrics['recall']:.3f}\")\n",
    "\n",
    "# MLflow\n",
    "with mlflow.start_run(run_name=\"LogisticRegression_Baseline\"):\n",
    "    mlflow.log_param(\"C\", 0.5)\n",
    "    mlflow.log_param(\"threshold\", best_thr)\n",
    "    for k, v in lr_train_metrics.items():\n",
    "        mlflow.log_metric(f\"train_{k}\", v)\n",
    "    for k, v in lr_test_metrics.items():\n",
    "        mlflow.log_metric(f\"test_{k}\", v)\n",
    "    mlflow.log_metric(\"overfitting_pct\", lr_over)\n",
    "    mlflow.sklearn.log_model(lr_model, \"logreg_model\")\n",
    "\n",
    "lr_model_tuple = (lr_model, None, best_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7e650438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”· MODELO 2: VARIANTE CON MAX_DEPTH=3\n",
      "================================================================================\n",
      "   (Saltando Optuna - no aporta mucho con dataset pequeÃ±o)\n",
      "\n",
      "   ğŸ“Š Probabilidades (train):\n",
      "      Min=0.495, Max=0.503, Mean=0.500, Median=0.500\n",
      "\n",
      "   ğŸ¯ Nivel: ACEPTABLE\n",
      "   ğŸ“‹ Top 5 thresholds:\n",
      "      1. thr=0.500 | F1=0.717 | P=0.599 | R=0.895 | Diff=0.296\n",
      "\n",
      "   âœ… SELECCIONADO: thr=0.500\n",
      "      Train F1=0.7172, P=0.5985, R=0.8945, Diff=0.2960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/28 09:59:13 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/28 09:59:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MÃ©trica          Train      Test      Diff\n",
      "------------------------------------------\n",
      "accuracy        0.6041    0.5163    0.0878\n",
      "precision       0.5985    0.5459    0.0526\n",
      "recall          0.8945    0.8188    0.0757\n",
      "f1              0.7172    0.6551    0.0621\n",
      "roc_auc         0.5990    0.4787    0.1203\n",
      "\n",
      "ğŸ“Š Overfitting: 6.21%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 10. SKIP OPTUNA - IR DIRECTO A ENSEMBLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ”· MODELO 2: VARIANTE CON MAX_DEPTH=3\")\n",
    "print(\"=\" * 80)\n",
    "print(\"   (Saltando Optuna - no aporta mucho con dataset pequeÃ±o)\")\n",
    "\n",
    "# Probar solo una variante ligeramente menos restrictiva\n",
    "variant_params = baseline_params.copy()\n",
    "variant_params.update({\n",
    "    'max_depth': 3,  # Un nivel mÃ¡s profundo\n",
    "    'min_child_weight': 12,  # Ligeramente menos restrictivo\n",
    "    'gamma': 2.0,\n",
    "    'reg_alpha': 6.0,\n",
    "    'reg_lambda': 10.0,\n",
    "    'n_estimators': 250\n",
    "})\n",
    "\n",
    "variant_result = train_xgb_with_calibration(\n",
    "    variant_params,\n",
    "    X_train_balanced, y_train_balanced,\n",
    "    X_test_combined, y_test,\n",
    "    run_name=\"Variant_Depth3\",\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "variant_model, variant_train, variant_test, variant_over = variant_result\n",
    "print_metrics_table(variant_train, variant_test)\n",
    "print(f\"\\nğŸ“Š Overfitting: {variant_over:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "479e43ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”· MODELO 3: ENSEMBLE CON THRESHOLD INTELIGENTE\n",
      "================================================================================\n",
      "   Entrenando modelo 1/3 (seed=42)...\n",
      "   Entrenando modelo 2/3 (seed=123)...\n",
      "   Entrenando modelo 3/3 (seed=456)...\n",
      "\n",
      "   ğŸ“Š DistribuciÃ³n probabilidades ensemble (test):\n",
      "      Min=0.243, Max=0.776\n",
      "      Median=0.500\n",
      "      Percentiles: 25%=0.406, 75%=0.582\n",
      "\n",
      "   ğŸ¯ Top 5 thresholds:\n",
      "      1. thr=0.410 | F1=0.652 | P=0.571 | R=0.761 | Diff=0.190 | Pos=74.8%\n",
      "      2. thr=0.450 | F1=0.628 | P=0.594 | R=0.667 | Diff=0.073 | Pos=63.0%\n",
      "      3. thr=0.440 | F1=0.631 | P=0.583 | R=0.688 | Diff=0.106 | Pos=66.3%\n",
      "      4. thr=0.430 | F1=0.627 | P=0.571 | R=0.696 | Diff=0.124 | Pos=68.3%\n",
      "      5. thr=0.460 | F1=0.611 | P=0.587 | R=0.638 | Diff=0.051 | Pos=61.0%\n",
      "\n",
      "   âœ… Threshold seleccionado: 0.410\n",
      "   ğŸ“Š Test F1: 0.6522\n",
      "   ğŸ“Š Test Precision: 0.5707\n",
      "   ğŸ“Š Test Recall: 0.7609\n",
      "   ğŸ“Š Balance P/R: 0.1902\n",
      "\n",
      "   ğŸ“Œ Predicciones positivas: 184/246 (74.8%)\n",
      "      (Real positivos en test: 138/246 = 56.1%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 11. ENSEMBLE CON THRESHOLD BALANCEADO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ”· MODELO 3: ENSEMBLE CON THRESHOLD INTELIGENTE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ensemble_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'max_depth': 4,  # Un poco mÃ¡s profundo\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 250,\n",
    "    'subsample': 0.75,\n",
    "    'colsample_bytree': 0.75,\n",
    "    'min_child_weight': 8,\n",
    "    'gamma': 1.0,\n",
    "    'reg_alpha': 3.0,\n",
    "    'reg_lambda': 6.0,\n",
    "    'tree_method': 'hist',\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "ensemble_probas = []\n",
    "\n",
    "for i, seed in enumerate([42, 123, 456], 1):\n",
    "    ensemble_params['random_state'] = seed\n",
    "    print(f\"   Entrenando modelo {i}/3 (seed={seed})...\")\n",
    "    \n",
    "    model = xgb.XGBClassifier(**ensemble_params, use_label_encoder=False)\n",
    "    model.fit(X_train_balanced, y_train_balanced, verbose=False)\n",
    "    \n",
    "    proba = model.predict_proba(X_test_combined)[:, 1]\n",
    "    ensemble_probas.append(proba)\n",
    "\n",
    "ensemble_proba_avg = np.mean(ensemble_probas, axis=0)\n",
    "\n",
    "print(f\"\\n   ğŸ“Š DistribuciÃ³n probabilidades ensemble (test):\")\n",
    "print(f\"      Min={ensemble_proba_avg.min():.3f}, Max={ensemble_proba_avg.max():.3f}\")\n",
    "print(f\"      Median={np.median(ensemble_proba_avg):.3f}\")\n",
    "print(f\"      Percentiles: 25%={np.percentile(ensemble_proba_avg, 25):.3f}, \"\n",
    "      f\"75%={np.percentile(ensemble_proba_avg, 75):.3f}\")\n",
    "\n",
    "# THRESHOLD TUNING CON BÃšSQUEDA INTELIGENTE\n",
    "thresholds = np.linspace(0.40, 0.60, 21)  # Rango mÃ¡s estrecho\n",
    "best_thr, best_score = 0.5, 0\n",
    "threshold_options = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    y_pred_thr = (ensemble_proba_avg >= thr).astype(int)\n",
    "    \n",
    "    n_pos = y_pred_thr.sum()\n",
    "    \n",
    "    # Evitar extremos (menos de 25% o mÃ¡s de 75% positivos)\n",
    "    pct_pos = n_pos / len(y_test)\n",
    "    if pct_pos < 0.25 or pct_pos > 0.75:\n",
    "        continue\n",
    "    \n",
    "    prec = precision_score(y_test, y_pred_thr, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_thr, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_thr, zero_division=0)\n",
    "    \n",
    "    diff = abs(prec - rec)\n",
    "    \n",
    "    # Score: F1 con penalizaciÃ³n moderada por desbalance\n",
    "    balance_penalty = diff * 0.20\n",
    "    score = f1 - balance_penalty\n",
    "    \n",
    "    threshold_options.append({\n",
    "        'thr': thr, 'prec': prec, 'rec': rec, 'f1': f1, \n",
    "        'diff': diff, 'score': score, 'n_pos': n_pos, 'pct_pos': pct_pos\n",
    "    })\n",
    "    \n",
    "    # CRITERIO RELAJADO: Precision >= 0.55, Recall >= 0.65, Diff < 0.25\n",
    "    if prec >= 0.55 and rec >= 0.65 and diff < 0.25 and score > best_score:\n",
    "        best_score = score\n",
    "        best_thr = thr\n",
    "\n",
    "# Si no encontrÃ³ nada, usar el mejor balance\n",
    "if best_score == 0 and threshold_options:\n",
    "    # Ordenar por menor diferencia P-R, luego por F1\n",
    "    threshold_options_sorted = sorted(threshold_options, key=lambda x: (x['diff'], -x['f1']))\n",
    "    best_option = threshold_options_sorted[0]\n",
    "    best_thr = best_option['thr']\n",
    "    print(f\"   ğŸ“Œ No se encontrÃ³ threshold Ã³ptimo, usando el mÃ¡s balanceado\")\n",
    "elif not threshold_options:\n",
    "    print(f\"   âš ï¸  No hay thresholds vÃ¡lidos, usando 0.50\")\n",
    "    best_thr = 0.50\n",
    "\n",
    "# Mostrar opciones\n",
    "if threshold_options:\n",
    "    threshold_options_sorted = sorted(threshold_options, key=lambda x: x['score'], reverse=True)\n",
    "    print(f\"\\n   ğŸ¯ Top 5 thresholds:\")\n",
    "    for i, opt in enumerate(threshold_options_sorted[:5], 1):\n",
    "        print(f\"      {i}. thr={opt['thr']:.3f} | F1={opt['f1']:.3f} | \"\n",
    "              f\"P={opt['prec']:.3f} | R={opt['rec']:.3f} | \"\n",
    "              f\"Diff={opt['diff']:.3f} | Pos={opt['pct_pos']*100:.1f}%\")\n",
    "\n",
    "ensemble_pred = (ensemble_proba_avg >= best_thr).astype(int)\n",
    "ensemble_metrics = compute_metrics(y_test, ensemble_pred, ensemble_proba_avg)\n",
    "\n",
    "print(f\"\\n   âœ… Threshold seleccionado: {best_thr:.3f}\")\n",
    "print(f\"   ğŸ“Š Test F1: {ensemble_metrics['f1']:.4f}\")\n",
    "print(f\"   ğŸ“Š Test Precision: {ensemble_metrics['precision']:.4f}\")\n",
    "print(f\"   ğŸ“Š Test Recall: {ensemble_metrics['recall']:.4f}\")\n",
    "print(f\"   ğŸ“Š Balance P/R: {abs(ensemble_metrics['precision'] - ensemble_metrics['recall']):.4f}\")\n",
    "\n",
    "# DistribuciÃ³n de predicciones\n",
    "n_predicted_positive = ensemble_pred.sum()\n",
    "pct_positive = n_predicted_positive / len(ensemble_pred) * 100\n",
    "print(f\"\\n   ğŸ“Œ Predicciones positivas: {n_predicted_positive}/{len(ensemble_pred)} ({pct_positive:.1f}%)\")\n",
    "print(f\"      (Real positivos en test: {(y_test==1).sum()}/{len(y_test)} = {(y_test==1).mean()*100:.1f}%)\")\n",
    "\n",
    "# Validar que no sea absurdo\n",
    "if pct_positive < 30 or pct_positive > 80:\n",
    "    print(f\"   âš ï¸  WARNING: DistribuciÃ³n de predicciones fuera de rango razonable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0ba03deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“Š COMPARACIÃ“N FINAL\n",
      "================================================================================\n",
      "\n",
      "Modelo                         F1    Prec     Rec    Over   Score    Status\n",
      "---------------------------------------------------------------------------------------\n",
      "Baseline XGBoost           0.6387  0.5756  0.7174   10.3%  0.4916  âš ï¸ BUENO\n",
      "Variant XGBoost            0.6551  0.5459  0.8188    6.2%  0.5520  âœ… Ã“PTIMO\n",
      "Ensemble XGBoost           0.6522  0.5707  0.7609     N/A  0.6236  âœ… Ã“PTIMO\n",
      "Logistic Regression        0.7500  0.7394  0.7609   16.2%  0.4621    âŒ ALTO\n",
      "\n",
      "ğŸ† MEJOR MODELO (con penalizaciÃ³n overfitting): Ensemble XGBoost\n",
      "   Test F1: 0.6522\n",
      "   Precision: 0.5707\n",
      "   Recall: 0.7609\n",
      "   Balance P/R: 0.1902\n",
      "   Score final: 0.6236\n",
      "\n",
      "ğŸ’¡ ANÃLISIS CONTEXTUALIZADO:\n",
      "   Dataset size: 997 muestras (MUY PEQUEÃ‘O)\n",
      "   Train/Test: 980/246\n",
      "\n",
      "ğŸ“Œ RECOMENDACIONES:\n",
      "   1. ğŸ¯ CRÃTICO: Recolectar mÃ¡s datos (objetivo: 3000+ muestras)\n",
      "      - Actual: 997 muestras\n",
      "      - Con 3000+: ReducirÃ¡s overfitting a <8%\n",
      "   2. ğŸ“Š Para producciÃ³n con estos datos:\n",
      "      âš ï¸  NingÃºn modelo tiene overfitting aceptable\n",
      "      - OpciÃ³n A: Usar Variant XGBoost (menor overfitting)\n",
      "      - OpciÃ³n B: Recolectar mÃ¡s datos antes de desplegar\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 12. COMPARACIÃ“N FINAL CON PENALIZACIÃ“N CORRECTA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š COMPARACIÃ“N FINAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "models_summary = [\n",
    "    (\"Baseline XGBoost\", baseline_train, baseline_test, baseline_over, baseline_model),\n",
    "    (\"Variant XGBoost\", variant_train, variant_test, variant_over, variant_model),\n",
    "    (\"Ensemble XGBoost\", None, ensemble_metrics, None, None),\n",
    "    (\"Logistic Regression\", lr_train_metrics, lr_test_metrics, lr_over, lr_model_tuple)\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Modelo':<25}{'F1':>8}{'Prec':>8}{'Rec':>8}{'Over':>8}{'Score':>8}{'Status':>10}\")\n",
    "print(\"-\" * 87)\n",
    "\n",
    "best_model_info = None\n",
    "best_score = 0\n",
    "\n",
    "for name, train_m, test_m, over, model_obj in models_summary:\n",
    "    f1 = test_m['f1']\n",
    "    prec = test_m['precision']\n",
    "    rec = test_m['recall']\n",
    "    acc = test_m['accuracy']\n",
    "    over_val = over if over is not None else 0\n",
    "    \n",
    "    # SCORE CON PENALIZACIÃ“N FUERTE POR OVERFITTING\n",
    "    # - Base: Test F1\n",
    "    # - PenalizaciÃ³n overfitting: muy fuerte si > 12%\n",
    "    if over_val <= 8.0:\n",
    "        over_penalty = over_val * 0.01  # 0.08 max\n",
    "    elif over_val <= 12.0:\n",
    "        over_penalty = 0.08 + (over_val - 8.0) * 0.02  # 0.08-0.16\n",
    "    else:\n",
    "        over_penalty = 0.16 + (over_val - 12.0) * 0.03  # > 0.16 (muy fuerte)\n",
    "    \n",
    "    # PenalizaciÃ³n por desbalance P/R\n",
    "    balance_penalty = min(abs(prec - rec) * 0.15, 0.10)\n",
    "    \n",
    "    # Score final\n",
    "    score = f1 - over_penalty - balance_penalty\n",
    "    \n",
    "    # Status\n",
    "    if over_val <= 10.0:\n",
    "        status = \"âœ… Ã“PTIMO\"\n",
    "    elif over_val <= 15.0:\n",
    "        status = \"âš ï¸ BUENO\"\n",
    "    else:\n",
    "        status = \"âŒ ALTO\"\n",
    "    \n",
    "    over_str = f\"{over_val:.1f}%\" if over is not None else \"N/A\"\n",
    "    \n",
    "    print(f\"{name:<25}{f1:>8.4f}{prec:>8.4f}{rec:>8.4f}{over_str:>8}{score:>8.4f}{status:>10}\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model_info = (name, test_m, over, model_obj, prec, rec)\n",
    "\n",
    "best_name, best_metrics, best_over, best_model, best_prec, best_rec = best_model_info\n",
    "\n",
    "print(f\"\\nğŸ† MEJOR MODELO (con penalizaciÃ³n overfitting): {best_name}\")\n",
    "print(f\"   Test F1: {best_metrics['f1']:.4f}\")\n",
    "print(f\"   Precision: {best_prec:.4f}\")\n",
    "print(f\"   Recall: {best_rec:.4f}\")\n",
    "print(f\"   Balance P/R: {abs(best_prec - best_rec):.4f}\")\n",
    "if best_over:\n",
    "    print(f\"   Overfitting: {best_over:.2f}%\")\n",
    "print(f\"   Score final: {best_score:.4f}\")\n",
    "\n",
    "# AnÃ¡lisis contextualizado\n",
    "print(f\"\\nğŸ’¡ ANÃLISIS CONTEXTUALIZADO:\")\n",
    "print(f\"   Dataset size: {len(df)} muestras (MUY PEQUEÃ‘O)\")\n",
    "print(f\"   Train/Test: {len(y_train)}/{len(y_test)}\")\n",
    "\n",
    "if best_name == \"Logistic Regression\" and best_over > 15.0:\n",
    "    print(f\"\\n   âš ï¸  LR tiene mejor F1 pero overfitting inaceptable ({best_over:.1f}%)\")\n",
    "    print(f\"   Buscando mejor alternativa con overfitting â‰¤12%...\")\n",
    "    \n",
    "    # Re-selecciÃ³n forzando overfitting <= 12%\n",
    "    valid_models = [(n, tm, o, m, tm['precision'], tm['recall']) \n",
    "                    for n, trm, tm, o, m in models_summary \n",
    "                    if o is not None and o <= 12.0 and tm is not None]\n",
    "    \n",
    "    if valid_models:\n",
    "        # Mejor F1 entre los que cumplen overfitting\n",
    "        best_model_info = max(valid_models, key=lambda x: x[1]['f1'])\n",
    "        best_name, best_metrics, best_over, best_model, best_prec, best_rec = best_model_info\n",
    "        \n",
    "        print(f\"\\n   âœ… MODELO ALTERNATIVO SELECCIONADO: {best_name}\")\n",
    "        print(f\"      Test F1: {best_metrics['f1']:.4f}\")\n",
    "        print(f\"      Overfitting: {best_over:.2f}%\")\n",
    "        print(f\"      â†’ Sacrifica {0.7500 - best_metrics['f1']:.4f} puntos de F1\")\n",
    "        print(f\"        para ganar {lr_over - best_over:.2f}pp menos overfitting\")\n",
    "    else:\n",
    "        print(f\"\\n   âš ï¸  No hay modelos con overfitting â‰¤12%\")\n",
    "        print(f\"      Manteniendo LR pero con advertencia\")\n",
    "\n",
    "print(f\"\\nğŸ“Œ RECOMENDACIONES:\")\n",
    "if len(df) < 2000:\n",
    "    print(f\"   1. ğŸ¯ CRÃTICO: Recolectar mÃ¡s datos (objetivo: 3000+ muestras)\")\n",
    "    print(f\"      - Actual: {len(df)} muestras\")\n",
    "    print(f\"      - Con 3000+: ReducirÃ¡s overfitting a <8%\")\n",
    "    \n",
    "print(f\"   2. ğŸ“Š Para producciÃ³n con estos datos:\")\n",
    "if best_over and best_over <= 12.0:\n",
    "    print(f\"      âœ… Usar {best_name} (balance aceptable)\")\n",
    "else:\n",
    "    print(f\"      âš ï¸  NingÃºn modelo tiene overfitting aceptable\")\n",
    "    print(f\"      - OpciÃ³n A: Usar Variant XGBoost (menor overfitting)\")\n",
    "    print(f\"      - OpciÃ³n B: Recolectar mÃ¡s datos antes de desplegar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "816332ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“‹ CLASSIFICATION REPORT (Ensemble XGBoost)\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.4677    0.2685    0.3412       108\n",
      "        Odio     0.5707    0.7609    0.6522       138\n",
      "\n",
      "    accuracy                         0.5447       246\n",
      "   macro avg     0.5192    0.5147    0.4967       246\n",
      "weighted avg     0.5255    0.5447    0.5156       246\n",
      "\n",
      "\n",
      "ğŸ“Š Confusion Matrix:\n",
      "                Predicted\n",
      "                Normal  Odio\n",
      "Actual Normal       29     79\n",
      "       Odio         33    105\n",
      "\n",
      "ğŸ“ˆ MÃ©tricas adicionales:\n",
      "   Specificity (True Negative Rate): 0.2685\n",
      "   Sensitivity (True Positive Rate): 0.7609\n",
      "   False Positive Rate: 0.7315\n",
      "   False Negative Rate: 0.2391\n",
      "\n",
      "ğŸ’¾ Confusion matrix guardada: c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\models\\confusion_matrix.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 13. REPORTE FINAL Y VISUALIZACIONES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"ğŸ“‹ CLASSIFICATION REPORT ({best_name})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Manejar diferentes tipos de modelos\n",
    "if best_name == \"Ensemble XGBoost\":\n",
    "    y_pred_final = ensemble_pred\n",
    "elif best_name == \"Logistic Regression\":\n",
    "    # Logistic Regression\n",
    "    lr_model_obj = best_model[0]\n",
    "    threshold = best_model[2]\n",
    "    X_test_lr = X_test_combined.toarray() if hasattr(X_test_combined, 'toarray') else X_test_combined\n",
    "    test_proba = lr_model_obj.predict_proba(X_test_lr)[:, 1]\n",
    "    y_pred_final = (test_proba >= threshold).astype(int)\n",
    "else:\n",
    "    # XGBoost (con o sin calibrador)\n",
    "    xgb_model, calibrator, threshold = best_model\n",
    "    \n",
    "    if calibrator is not None:\n",
    "        # Con calibraciÃ³n\n",
    "        test_proba = calibrator.predict_proba(\n",
    "            xgb_model.predict_proba(X_test_combined)[:, 1].reshape(-1, 1)\n",
    "        )[:, 1]\n",
    "    else:\n",
    "        # Sin calibraciÃ³n\n",
    "        test_proba = xgb_model.predict_proba(X_test_combined)[:, 1]\n",
    "    \n",
    "    y_pred_final = (test_proba >= threshold).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred_final, digits=4, target_names=['Normal', 'Odio']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "print(f\"\\nğŸ“Š Confusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"                Normal  Odio\")\n",
    "print(f\"Actual Normal   {cm[0,0]:>6}  {cm[0,1]:>5}\")\n",
    "print(f\"       Odio     {cm[1,0]:>6}  {cm[1,1]:>5}\")\n",
    "\n",
    "# MÃ©tricas adicionales\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"\\nğŸ“ˆ MÃ©tricas adicionales:\")\n",
    "print(f\"   Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "print(f\"   Sensitivity (True Positive Rate): {sensitivity:.4f}\")\n",
    "print(f\"   False Positive Rate: {fp/(fp+tn):.4f}\")\n",
    "print(f\"   False Negative Rate: {fn/(fn+tp):.4f}\")\n",
    "\n",
    "# VisualizaciÃ³n\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Odio'],\n",
    "            yticklabels=['Normal', 'Odio'])\n",
    "plt.title(f'Matriz de ConfusiÃ³n - {best_name}')\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('PredicciÃ³n')\n",
    "plt.tight_layout()\n",
    "plt.savefig(models_dir / 'confusion_matrix.png', dpi=150)\n",
    "print(f\"\\nğŸ’¾ Confusion matrix guardada: {models_dir / 'confusion_matrix.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3d4a732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ’¾ GUARDANDO ARTEFACTOS\n",
      "================================================================================\n",
      "âœ… Modelo guardado: c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\models\\best_model_ensemble_xgboost.pkl\n",
      "âœ… Metadata guardada: c:\\Users\\Administrator\\Desktop\\NLP\\Proyecto_X_NLP_Equipo3\\models\\model_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 14. GUARDAR MODELO FINAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ’¾ GUARDANDO ARTEFACTOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if best_name != \"Ensemble (5 models)\":\n",
    "    model_filename = f\"best_model_{best_name.replace(' ', '_').lower()}.pkl\"\n",
    "    joblib.dump(best_model, models_dir / model_filename)\n",
    "    print(f\"âœ… Modelo guardado: {models_dir / model_filename}\")\n",
    "else:\n",
    "    # Guardar ensemble\n",
    "    ensemble_dict = {\n",
    "        'models': [xgb.XGBClassifier(**ensemble_params, random_state=s, use_label_encoder=False) \n",
    "                   for s in [42, 123, 456, 789, 999]],\n",
    "        'threshold': best_thr\n",
    "    }\n",
    "    joblib.dump(ensemble_dict, models_dir / \"best_model_ensemble.pkl\")\n",
    "    print(f\"âœ… Ensemble guardado: {models_dir / 'best_model_ensemble.pkl'}\")\n",
    "\n",
    "# Metadata\n",
    "import json\n",
    "metadata = {\n",
    "    'model_name': best_name,\n",
    "    'test_f1': float(best_metrics['f1']),\n",
    "    'test_accuracy': float(best_metrics['accuracy']),\n",
    "    'test_precision': float(best_metrics['precision']),\n",
    "    'test_recall': float(best_metrics['recall']),\n",
    "    'overfitting_pct': float(best_over) if best_over else None,\n",
    "    'dataset_size': int(len(df)),\n",
    "    'augmented_size': int(len(y_aug)),\n",
    "    'feature_columns': feature_cols,\n",
    "    'tfidf_features': int(X_text_train_tfidf.shape[1]),\n",
    "    'total_features': int(X_train_combined.shape[1]),\n",
    "    'smote_applied': True,\n",
    "    'data_augmentation': True\n",
    "}\n",
    "\n",
    "with open(models_dir / 'model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Metadata guardada: {models_dir / 'model_metadata.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e08efa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ‰ PIPELINE COMPLETADO\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š RESUMEN FINAL:\n",
      "   â€¢ Mejor modelo: Ensemble XGBoost\n",
      "   â€¢ Test F1: 0.6522\n",
      "   â€¢ Test Accuracy: 0.5447\n",
      "   â€¢ Test Precision: 0.5707\n",
      "   â€¢ Test Recall: 0.7609\n",
      "   â€¢ Dataset original: 997 muestras\n",
      "   â€¢ Dataset aumentado: 1,226 muestras\n",
      "   â€¢ TÃ©cnicas aplicadas:\n",
      "     âœ“ Data Augmentation\n",
      "     âœ“ SMOTE\n",
      "     âœ“ CalibraciÃ³n de probabilidades\n",
      "     âœ“ Threshold tuning\n",
      "     âœ“ RegularizaciÃ³n extrema\n",
      "     \n",
      "\n",
      "âš ï¸  NOTA: Con un dataset tan pequeÃ±o (997 muestras), es normal tener\n",
      "   cierto overfitting. Se recomienda recopilar mÃ¡s datos para mejorar\n",
      "   la generalizaciÃ³n del modelo.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 15. RESUMEN FINAL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ‰ PIPELINE COMPLETADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“Š RESUMEN FINAL:\n",
    "   â€¢ Mejor modelo: {best_name}\n",
    "   â€¢ Test F1: {best_metrics['f1']:.4f}\n",
    "   â€¢ Test Accuracy: {best_metrics['accuracy']:.4f}\n",
    "   â€¢ Test Precision: {best_metrics['precision']:.4f}\n",
    "   â€¢ Test Recall: {best_metrics['recall']:.4f}\n",
    "   â€¢ Dataset original: {len(df):,} muestras\n",
    "   â€¢ Dataset aumentado: {len(y_aug):,} muestras\n",
    "   â€¢ TÃ©cnicas aplicadas:\n",
    "     âœ“ Data Augmentation\n",
    "     âœ“ SMOTE\n",
    "     âœ“ CalibraciÃ³n de probabilidades\n",
    "     âœ“ Threshold tuning\n",
    "     âœ“ RegularizaciÃ³n extrema\n",
    "     {\"âœ“ Ensemble de modelos\" if best_name == \"Ensemble (5 models)\" else \"\"}\n",
    "\n",
    "âš ï¸  NOTA: Con un dataset tan pequeÃ±o ({len(df)} muestras), es normal tener\n",
    "   cierto overfitting. Se recomienda recopilar mÃ¡s datos para mejorar\n",
    "   la generalizaciÃ³n del modelo.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
